{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peaceful_pie.unity_comms import UnityComms\n",
    "import argparse\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Box, MultiBinary,Discrete\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers import FrameStack\n",
    "from collections import deque\n",
    "# Import UnityComms from peaceful_pie.unity_comms\n",
    "from peaceful_pie.unity_comms import UnityComms\n",
    "from peaceful_pie.unity_comms import UnityComms\n",
    "from peaceful_pie import ray_results_helper\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import retro\n",
    "import cv2\n",
    "from gym.spaces import Box,MultiBinary\n",
    "import numpy as np\n",
    "import optuna\n",
    "import tqdm as notebook_tqdm\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder ,VecFrameStack\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unity_comms(port: int):\n",
    "    unity_comms = UnityComms(port)\n",
    "    return unity_comms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "port = 5000  # Replace with your desired port number\n",
    "unity_comms_instance = unity_comms(port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "unity_comms_instance \n",
    "unity_comms = unity_comms_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVector3:\n",
    "    def __init__(self, x, y, z):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.z = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class UnityEnv(Env):\n",
    "    def __init__(self, unity_comms, i):\n",
    "        self.unity_comms = unity_comms\n",
    "        self.action_space = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])  # 15 possible actions\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(3,), dtype=np.float32)\n",
    "        self.initial_position = None\n",
    "        self.prev_position = None\n",
    "        self.prev_velocity = 0\n",
    "        self.frame_count = 0\n",
    "        self.i = i\n",
    "\n",
    "\n",
    "    def action(self):\n",
    "        actions = {\n",
    "            0: [np.array([1.0, 0.0, 0.0])],  # GoForward\n",
    "            1: [np.array([-1.0, 0.0, 0.0])],  # GoReverse\n",
    "            2: [np.array([0.0, 1.0, 0.0])],  # TurnLeft\n",
    "            3: [np.array([0.0, -1.0, 0.0])],  # TurnRight\n",
    "            4: [np.array([0.0, 0.0, 1.0])],  # Handbrake\n",
    "            5: [np.array([1.0, 1.0, 0.0])],  # GoForward + TurnLeft\n",
    "            6: [np.array([1.0, -1.0, 0.0])],  # GoForward + TurnRight\n",
    "            7: [np.array([1.0, 0.0, 1.0])],  # GoForward + Handbrake\n",
    "            8: [np.array([-1.0, 1.0, 0.0])],  # GoReverse + TurnLeft\n",
    "            9: [np.array([-1.0, -1.0, 0.0])],  # GoReverse + TurnRight\n",
    "            10: [np.array([-1.0, 0.0, 1.0])],  # GoReverse + Handbrake\n",
    "            11: [np.array([1.0, 1.0, 1.0])],  # GoForward + TurnLeft + Handbrake\n",
    "            12: [np.array([1.0, -1.0, 1.0])],  # GoForward + TurnRight + Handbrake\n",
    "            13: [np.array([-1.0, 1.0, 1.0])],  # GoReverse + TurnLeft + Handbrake\n",
    "            14: [np.array([-1.0, -1.0, 1.0])]  # GoReverse + TurnRight + Handbrake\n",
    "        }\n",
    "        return actions\n",
    "\n",
    "    def step(self, action):\n",
    "        if action.item() in self.action_space:\n",
    "            action_methods = self.action()[action.item()]\n",
    "            for method in action_methods:\n",
    "                action_method = method.tolist()\n",
    "                if action_method == [1.0, 0.0, 0.0]:\n",
    "                    getattr(self.unity_comms, f\"GoForward_{self.i}\")()\n",
    "                elif action_method == [-1.0, 0.0, 0.0]:\n",
    "                    getattr(self.unity_comms, f\"GoReverse_{self.i}\")()\n",
    "                elif action_method == [0.0, 1.0, 0.0]:\n",
    "                    getattr(self.unity_comms, f\"TurnLeft_{self.i}\")()\n",
    "                elif action_method == [0.0, -1.0, 0.0]:\n",
    "                    getattr(self.unity_comms, f\"TurnRight_{self.i}\")()\n",
    "                elif action_method == [0.0, 0.0, 1.0]:\n",
    "                    getattr(self.unity_comms, f\"Handbrake_{self.i}\")()\n",
    "                elif action_method == [1.0, 1.0, 0.0]:\n",
    "                    getattr(self.unity_comms, f\"GoForward_{self.i}\")()\n",
    "                    getattr(self.unity_comms, f\"TurnLeft_{self.i}\")()\n",
    "                elif action_method == [1.0, -1.0, 0.0]:\n",
    "                    getattr(self.unity_comms, f\"GoForward_{self.i}\")()\n",
    "                    getattr(self.unity_comms, f\"TurnRight_{self.i}\")()\n",
    "                elif action_method == [1.0, 0.0, 1.0]:\n",
    "                    getattr(self.unity_comms, f\"GoForward_{self.i}\")()\n",
    "                    getattr(self.unity_comms, f\"Handbrake_{self.i}\")()\n",
    "                elif action_method == [-1.0, 1.0, 0.0]:\n",
    "                    getattr(self.unity_comms, f\"GoReverse_{self.i}\")()\n",
    "                    getattr(self.unity_comms, f\"TurnLeft_{self.i}\")()\n",
    "                elif action_method == [-1.0, -1.0, 0.0]:\n",
    "                    getattr(self.unity_comms, f\"GoReverse_{self.i}\")()\n",
    "                    getattr(self.unity_comms, f\"TurnRight_{self.i}\")()\n",
    "                elif action_method == [-1.0, 0.0, 1.0]:\n",
    "                    getattr(self.unity_comms, f\"GoReverse_{self.i}\")()\n",
    "                    getattr(self.unity_comms, f\"Handbrake_{self.i}\")()\n",
    "                elif action_method == [1.0, 1.0, 1.0]:\n",
    "                    getattr(self.unity_comms, f\"GoForward_{self.i}\")()\n",
    "                    getattr(self.unity_comms, f\"TurnLeft_{self.i}\")()\n",
    "                    getattr(self.unity_comms, f\"Handbrake_{self.i}\")()\n",
    "                elif action_method == [1.0, -1.0, 1.0]:\n",
    "                    getattr(self.unity_comms, f\"GoForward_{self.i}\")()\n",
    "                    getattr(self.unity_comms, f\"TurnRight_{self.i}\")()\n",
    "                    getattr(self.unity_comms, f\"Handbrake_{self.i}\")()\n",
    "                elif action_method == [-1.0, 1.0, 1.0]:\n",
    "                    getattr(self.unity_comms, f\"GoReverse_{self.i}\")()\n",
    "                    getattr(self.unity_comms, f\"TurnLeft_{self.i}\")()\n",
    "                    getattr(self.unity_comms, f\"Handbrake_{self.i}\")()\n",
    "                elif action_method == [-1.0, -1.0, 1.0]:\n",
    "                    getattr(self.unity_comms, f\"GoReverse_{self.i}\")()\n",
    "                    getattr(self.unity_comms, f\"TurnRight_{self.i}\")()\n",
    "                    getattr(self.unity_comms, f\"Handbrake_{self.i}\")()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action index\")\n",
    "\n",
    "\n",
    "        self.prev_position = self.Get_position()\n",
    "        self.prev_velocity = self.Get_velocity()\n",
    "\n",
    "        position = self.Get_position()\n",
    "        velocity = self.Get_velocity()\n",
    "        # Concatenate the position and velocity to form the observation\n",
    "        observation = np.array([position[0], position[1], velocity], dtype=np.float32)\n",
    "\n",
    "        reward = self.Get_reward()\n",
    "        done = self.done()\n",
    "\n",
    "        # Update the info dictionary with relevant information\n",
    "        info = {\n",
    "            'episode_reward': reward,  # Replace with the actual episode reward\n",
    "            'episode_length': self.frame_count,  # Replace with the actual episode length\n",
    "            'current_observation': observation,\n",
    "            'action_taken': action,\n",
    "            'obstacle_visible': self.Is_obstacle_visible(),\n",
    "            'reward_visible': self.Is_reward_visible(),\n",
    "            'goal_reached': self.Goal(),\n",
    "            'car_collision_detected': self.Get_carCollisionDetected(),\n",
    "            'velocity_incremented': self.Check_valocity_increment(),\n",
    "        }\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def RayCast(self):\n",
    "        ray_results = getattr(self.unity_comms, f\"GetRayCastsResults_{self.i}\")()\n",
    "        distance = np.array(ray_results['rayDistances'], dtype=np.float32)\n",
    "        types = np.array(ray_results['rayHitObjectTypes'], dtype=np.int32)\n",
    "        num_types = ray_results['NumObjectTypes']\n",
    "        self.actual_result = ray_results_helper.ray_results_to_feature_np(\n",
    "            ray_results_helper.RayResults(\n",
    "                NumObjectTypes=num_types,\n",
    "                rayDistances=distance,\n",
    "                rayHitObjectTypes=types,\n",
    "            )\n",
    "        )  \n",
    "\n",
    "        return self.actual_result\n",
    "    \n",
    "\n",
    "    def Is_obstacle_visible(self):\n",
    "        actual_result=self.RayCast()\n",
    "        obstacle_channel = actual_result[0]  # Extract the obstacle channel from the actual result\n",
    "        if np.max(obstacle_channel) > 0:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def Is_reward_visible(self):\n",
    "        actual_result=self.RayCast()\n",
    "        reward_channel = actual_result[1]  # Extract the reward channel from the actual result\n",
    "        if np.max(reward_channel) > 0:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def Goal(self):\n",
    "        if self.Is_reward_visible() == True:\n",
    "            goal_channel = self.actual_result[1]\n",
    "            if np.max(goal_channel) > 1:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def Get_reward(self):\n",
    "        obstacle_penalty = -1.0  # Penalty for encountering an obstacle\n",
    "        reward_bonus = .7 # Reward for finding a reward object\n",
    "        goal_reward = 1  # Reward for reaching the goal\n",
    "        car_collision_penalty = -1.0  # Penalty for colliding with the car\n",
    "        valocity_reward = 0.1 #reward for moving forward\n",
    "\n",
    "        reward = 0.0\n",
    "\n",
    "        if self.Is_obstacle_visible()== True:\n",
    "            reward += obstacle_penalty\n",
    "\n",
    "        if self.Is_reward_visible()== True:\n",
    "            reward += reward_bonus\n",
    "\n",
    "        reward += self.Step_panalty()\n",
    "\n",
    "        if self.Goal() == True:\n",
    "            reward += goal_reward\n",
    "\n",
    "        if self.Get_carCollisionDetected() == True:\n",
    "            reward += car_collision_penalty\n",
    "\n",
    "        if self.Check_valocity_increment()== True:\n",
    "            reward += valocity_reward\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def reset(self):\n",
    "        self.RayCast()\n",
    "        Reset_Car = getattr(self.unity_comms, f\"ResetPosition_{self.i}\")()  \n",
    "        Reset_Agent = getattr(self.unity_comms, f\"ResetPosition_Plane_{self.i}\")()\n",
    "        position =self.Get_position()\n",
    "        position = np.array(position, dtype=np.float32)\n",
    "        self.frame_count=0\n",
    "        return position\n",
    "\n",
    "    def done(self):\n",
    "        if self.Get_carCollisionDetected() == True:\n",
    "            self.frame_count=0\n",
    "            return True\n",
    "        elif self.Get_movingPlaneCollision() == True:\n",
    "            self.frame_count=0\n",
    "            return True\n",
    "        elif self.Get_planeCollision() == True:\n",
    "            self.frame_count=0\n",
    "            return True\n",
    "        elif self.Check_stuck() == True:\n",
    "            self.frame_count=0\n",
    "            return True\n",
    "        else:\n",
    "            self.frame_count += 1\n",
    "            return False\n",
    "\n",
    "    def Get_carCollisionDetected(self):\n",
    "        collision_count = 0\n",
    "        collision = collision = getattr(self.unity_comms, f\"CarCollisionDetected_{self.i}\")()\n",
    "        if collision == 1:\n",
    "            collision_count += 1\n",
    "            if collision_count >=2:\n",
    "                collision_count = 0\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    def Get_movingPlaneCollision(self):\n",
    "        collision_count = 0\n",
    "        collision = getattr(self.unity_comms, f\"GetMovingPlaneCollision_{self.i}\")()\n",
    "        if collision == 1:\n",
    "            collision_count += 1\n",
    "            if collision_count >= 1:\n",
    "                collision_count = 0\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    def Get_planeCollision(self):\n",
    "        collision_count = 0\n",
    "        collision = getattr(self.unity_comms, f\"GetPlaneCollision_{self.i}\")()\n",
    "        if collision == 1:\n",
    "            collision_count += 1\n",
    "            if collision_count > 0:\n",
    "                collision_count = 0\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    def Get_rewardCollision(self):\n",
    "        collision = getattr(self.unity_comms, f\"GetRewardCollision_{self.i}\")()\n",
    "        self.rewardcollision = collision\n",
    "\n",
    "    def Step_panalty(self):\n",
    "        if self.done() == True:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "    def Get_velocity(self):\n",
    "        valocity =  getattr(self.unity_comms, f\"CarSpeedUI_{self.i}\")()\n",
    "        return valocity\n",
    "\n",
    "    def Check_valocity_increment(self):\n",
    "        valocity = self.Get_velocity()\n",
    "        if valocity > self.prev_velocity:\n",
    "            self.prev_velocity = valocity\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def Check_stuck(self):\n",
    "        position_threshold = 0.01\n",
    "        consecutive_steps = 20\n",
    "        position_counter = 0\n",
    "        \n",
    "        if self.prev_position is None:\n",
    "            self.prev_position = self.get_position()\n",
    "\n",
    "        for _ in range(consecutive_steps):\n",
    "            x, y, z = self.Get_position()\n",
    "\n",
    "            position_diff = np.linalg.norm(np.array([x, y, z]) - np.array(self.prev_position))\n",
    "            if position_diff < position_threshold:\n",
    "                position_counter += 1\n",
    "            else:\n",
    "                position_counter = 0\n",
    "\n",
    "            self.prev_position = [x, y, z]\n",
    "\n",
    "        if position_counter >= consecutive_steps:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def Get_position(self):\n",
    "        position = getattr(self.unity_comms, f\"GetPosition_{self.i}\")()\n",
    "        # Extract x, y, and z components from position\n",
    "        x = position['x']\n",
    "        y = position['y']\n",
    "        z = position['z']\n",
    "        return x, y, z\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_env=UnityEnv(unity_comms,5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requests.exceptions.ConnectionError => ignoring, retrying\n",
      "requests.exceptions.ConnectionError => ignoring, retrying\n",
      "requests.exceptions.ConnectionError => ignoring, retrying\n",
      "requests.exceptions.ConnectionError => ignoring, retrying\n",
      "requests.exceptions.ConnectionError => ignoring, retrying\n",
      "requests.exceptions.ConnectionError => ignoring, retrying\n",
      "requests.exceptions.ConnectionError => ignoring, retrying\n",
      "requests.exceptions.ConnectionError => ignoring, retrying\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_env.RayCast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "file_path = \"dataset.h5\"\n",
    "hdf5_file = h5py.File(file_path, \"r\")\n",
    "\n",
    "# Load the dataset into a variable\n",
    "dataset = hdf5_file[\"nested_arrays_dataset\"]\n",
    "\n",
    "# Access the data as a NumPy array\n",
    "nested_arrays = dataset[:]\n",
    "\n",
    "# Close the HDF5 file\n",
    "hdf5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2, 70, 20)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nested_arrays.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 20) (70, 20)\n"
     ]
    }
   ],
   "source": [
    "nested_arrays = np.array(nested_arrays)\n",
    "x=nested_arrays[9999][0].shape#last obstacle\n",
    "y=nested_arrays[9999][1].shape#last reward\n",
    "plt.imshow(nested_arrays[9999][1])\n",
    "plt.show()\n",
    "plt.imshow(nested_arrays[9999][0])\n",
    "plt.show()\n",
    "print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class RayCastDataset(Dataset):\n",
    "    def __init__(self, nested_arrays):\n",
    "        self.nested_arrays = nested_arrays\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.nested_arrays)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        obstacle = self.nested_arrays[idx][0]\n",
    "        reward = self.nested_arrays[idx][1]\n",
    "\n",
    "        obstacle_tensor = torch.from_numpy(obstacle).float()\n",
    "        reward_tensor = torch.from_numpy(reward).float()\n",
    "\n",
    "        return obstacle_tensor, reward_tensor\n",
    "dataset = RayCastDataset(nested_arrays)\n",
    "obstacle, reward = dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "obstacle_0 = obstacle[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70, 20])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obstacle_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NestedCNN(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super(NestedCNN, self).__init__()\n",
    "        self.n = n\n",
    "        self.cnn_module = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(448, 1),\n",
    "            nn.ReLU()  # Remove the Sigmoid activation from here\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        outputs = []\n",
    "        for i in range(self.n):\n",
    "            nested_input = input[:, i, :, :].unsqueeze(1)\n",
    "            output = nested_input\n",
    "            # print(\"Nested Input shape:\", output.shape)\n",
    "            \n",
    "            output = self.cnn_module[0](output)\n",
    "            # print(\"Conv2d 1 Output shape:\", output.shape)\n",
    "            \n",
    "            output = self.cnn_module[1](output)\n",
    "            # print(\"ReLU 1 Output shape:\", output.shape)\n",
    "            \n",
    "            output = self.cnn_module[2](output)\n",
    "            # print(\"MaxPool2d 1 Output shape:\", output.shape)\n",
    "            \n",
    "            output = self.cnn_module[3](output)\n",
    "            # print(\"Conv2d 2 Output shape:\", output.shape)\n",
    "            \n",
    "            output = self.cnn_module[4](output)\n",
    "            # print(\"ReLU 2 Output shape:\", output.shape)\n",
    "            \n",
    "            output = self.cnn_module[5](output)\n",
    "            # print(\"MaxPool2d 2 Output shape:\", output.shape)\n",
    "            \n",
    "            output = self.cnn_module[6](output)\n",
    "            # print(\"Conv2d 3 Output shape:\", output.shape)\n",
    "            \n",
    "            output = self.cnn_module[7](output)\n",
    "            # print(\"ReLU 3 Output shape:\", output.shape)\n",
    "            \n",
    "            output = self.cnn_module[8](output)\n",
    "            # print(\"MaxPool2d 3 Output shape:\", output.shape)\n",
    "            \n",
    "            output = self.cnn_module[9](output)\n",
    "            # print(\"Flatten Output shape:\", output.shape)\n",
    "\n",
    "            \n",
    "            output = self.cnn_module[10](output)\n",
    "            # print(\"Linear 1 Output shape:\", output.shape)\n",
    "            output = nn.Sigmoid()(output)\n",
    "            # print(\"Sigmoid Output shape:\", output.shape)\n",
    "            \n",
    "            outputs.append(output)\n",
    "        \n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NestedCNN(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "NestedCNN                                [1, 2, 1]                 --\n",
       "├─Sequential: 1-1                        --                        --\n",
       "│    └─Conv2d: 2-1                       [1, 16, 68, 18]           160\n",
       "│    └─ReLU: 2-2                         [1, 16, 68, 18]           --\n",
       "│    └─MaxPool2d: 2-3                    [1, 16, 34, 9]            --\n",
       "│    └─Conv2d: 2-4                       [1, 32, 32, 7]            4,640\n",
       "│    └─ReLU: 2-5                         [1, 32, 32, 7]            --\n",
       "│    └─MaxPool2d: 2-6                    [1, 32, 16, 3]            --\n",
       "│    └─Conv2d: 2-7                       [1, 64, 14, 1]            18,496\n",
       "│    └─ReLU: 2-8                         [1, 64, 14, 1]            --\n",
       "│    └─MaxPool2d: 2-9                    [1, 64, 7, 1]             --\n",
       "│    └─Flatten: 2-10                     [1, 448]                  --\n",
       "│    └─Linear: 2-11                      [1, 1]                    449\n",
       "│    └─Conv2d: 2-12                      [1, 16, 68, 18]           (recursive)\n",
       "│    └─ReLU: 2-13                        [1, 16, 68, 18]           --\n",
       "│    └─MaxPool2d: 2-14                   [1, 16, 34, 9]            --\n",
       "│    └─Conv2d: 2-15                      [1, 32, 32, 7]            (recursive)\n",
       "│    └─ReLU: 2-16                        [1, 32, 32, 7]            --\n",
       "│    └─MaxPool2d: 2-17                   [1, 32, 16, 3]            --\n",
       "│    └─Conv2d: 2-18                      [1, 64, 14, 1]            (recursive)\n",
       "│    └─ReLU: 2-19                        [1, 64, 14, 1]            --\n",
       "│    └─MaxPool2d: 2-20                   [1, 64, 7, 1]             --\n",
       "│    └─Flatten: 2-21                     [1, 448]                  --\n",
       "│    └─Linear: 2-22                      [1, 1]                    (recursive)\n",
       "==========================================================================================\n",
       "Total params: 23,745\n",
       "Trainable params: 23,745\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 2.99\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.44\n",
       "Params size (MB): 0.09\n",
       "Estimated Total Size (MB): 0.55\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model=model,input_size=(1,2,70,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 70, 20])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obstacle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 70, 20])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obstacle = obstacle.unsqueeze(0)  # Add batch dimension\n",
    "obstacle = obstacle.unsqueeze(1)  # Add nested dimension\n",
    "obstacle.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\RL_UNITY\\full_car_envs\\my_experiment\\Assets\\subAssets\\Python\\environment\\Env5000\\Torch_CNN.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/Torch_CNN.ipynb#Y355sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m obstacle_0\u001b[39m.\u001b[39mto(device)  \u001b[39m# Move to GPU\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/Torch_CNN.ipynb#Y355sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Pass the obstacle tensor to the model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/Torch_CNN.ipynb#Y355sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m pred \u001b[39m=\u001b[39m model(obstacle_0)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\RL_UNITY\\full_car_envs\\my_experiment\\Assets\\subAssets\\Python\\environment\\Env5000\\Torch_CNN.ipynb Cell 26\u001b[0m in \u001b[0;36mNestedCNN.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/Torch_CNN.ipynb#Y355sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m outputs \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/Torch_CNN.ipynb#Y355sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/Torch_CNN.ipynb#Y355sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     nested_input \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m[:, i, :, :]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/Torch_CNN.ipynb#Y355sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     output \u001b[39m=\u001b[39m nested_input\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/Torch_CNN.ipynb#Y355sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m# print(\"Nested Input shape:\", output.shape)\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "\n",
    "obstacle_0.to(device)  # Move to GPU\n",
    "# Pass the obstacle tensor to the model\n",
    "pred = model(obstacle_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wall_data_loader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NestedCNN(2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= torch.randn(1,2,70,20).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\RL_UNITY\\full_car_envs\\my_experiment\\Assets\\subAssets\\Python\\environment\\Env5000\\Torch_CNN.ipynb Cell 33\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/Torch_CNN.ipynb#Y345sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m prob\u001b[39m=\u001b[39mmodel(wall_data_loader\u001b[39m.\u001b[39;49mdataset[\u001b[39m0\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\RL_UNITY\\full_car_envs\\my_experiment\\Assets\\subAssets\\Python\\environment\\Env5000\\Torch_CNN.ipynb Cell 33\u001b[0m in \u001b[0;36mNestedCNN.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/Torch_CNN.ipynb#Y345sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m outputs \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/Torch_CNN.ipynb#Y345sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/Torch_CNN.ipynb#Y345sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     nested_input \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m[:, i, :, :]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/Torch_CNN.ipynb#Y345sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     output \u001b[39m=\u001b[39m nested_input\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/Torch_CNN.ipynb#Y345sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m# print(\"Nested Input shape:\", output.shape)\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "prob=model(wall_data_loader.dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5205],\n",
       "         [0.5137]]], device='cuda:0', grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70, 20])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wall_data_loader.dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\RL_UNITY\\full_car_envs\\my_experiment\\Assets\\subAssets\\Python\\environment\\Env5000\\Torch_CNN.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/Torch_CNN.ipynb#Y335sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m wall_data_loader:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/Torch_CNN.ipynb#Y335sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     batch \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/Torch_CNN.ipynb#Y335sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     probs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(batch)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/Torch_CNN.ipynb#Y335sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     outputs\u001b[39m.\u001b[39mappend(probs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/Torch_CNN.ipynb#Y335sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(outputs, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;32md:\\RL_UNITY\\full_car_envs\\my_experiment\\Assets\\subAssets\\Python\\environment\\Env5000\\Torch_CNN.ipynb Cell 24\u001b[0m in \u001b[0;36mNestedCNN.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/Torch_CNN.ipynb#Y335sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m outputs \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/Torch_CNN.ipynb#Y335sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/Torch_CNN.ipynb#Y335sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     nested_input \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m[:, i, :, :]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/Torch_CNN.ipynb#Y335sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     output \u001b[39m=\u001b[39m nested_input\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/Torch_CNN.ipynb#Y335sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m# print(\"Nested Input shape:\", output.shape)\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 3"
     ]
    }
   ],
   "source": [
    "model = NestedCNN(2).to(device)\n",
    "outputs = []\n",
    "\n",
    "for batch in wall_data_loader:\n",
    "    batch = batch.to(device)\n",
    "    probs = model.forward(batch)\n",
    "    outputs.append(probs)\n",
    "\n",
    "outputs = torch.cat(outputs, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 2, 70, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    }
   ],
   "source": [
    "cnn = NestedCNN(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nested Input shape: torch.Size([1, 1, 70, 20])\n",
      "Conv2d 1 Output shape: torch.Size([1, 16, 68, 18])\n",
      "ReLU 1 Output shape: torch.Size([1, 16, 68, 18])\n",
      "MaxPool2d 1 Output shape: torch.Size([1, 16, 34, 9])\n",
      "Conv2d 2 Output shape: torch.Size([1, 32, 32, 7])\n",
      "ReLU 2 Output shape: torch.Size([1, 32, 32, 7])\n",
      "MaxPool2d 2 Output shape: torch.Size([1, 32, 16, 3])\n",
      "Conv2d 3 Output shape: torch.Size([1, 64, 14, 1])\n",
      "ReLU 3 Output shape: torch.Size([1, 64, 14, 1])\n",
      "MaxPool2d 3 Output shape: torch.Size([1, 64, 7, 1])\n",
      "Flatten Output shape: torch.Size([1, 448])\n",
      "Linear 1 Output shape: torch.Size([1, 1])\n",
      "ReLU 4 Output shape: torch.Size([1, 1])\n",
      "sigmoid_Final Output shape: torch.Size([1, 1])\n",
      "Nested Input shape: torch.Size([1, 1, 70, 20])\n",
      "Conv2d 1 Output shape: torch.Size([1, 16, 68, 18])\n",
      "ReLU 1 Output shape: torch.Size([1, 16, 68, 18])\n",
      "MaxPool2d 1 Output shape: torch.Size([1, 16, 34, 9])\n",
      "Conv2d 2 Output shape: torch.Size([1, 32, 32, 7])\n",
      "ReLU 2 Output shape: torch.Size([1, 32, 32, 7])\n",
      "MaxPool2d 2 Output shape: torch.Size([1, 32, 16, 3])\n",
      "Conv2d 3 Output shape: torch.Size([1, 64, 14, 1])\n",
      "ReLU 3 Output shape: torch.Size([1, 64, 14, 1])\n",
      "MaxPool2d 3 Output shape: torch.Size([1, 64, 7, 1])\n",
      "Flatten Output shape: torch.Size([1, 448])\n",
      "Linear 1 Output shape: torch.Size([1, 1])\n",
      "ReLU 4 Output shape: torch.Size([1, 1])\n",
      "sigmoid_Final Output shape: torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "prob = cnn_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob=prob.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000],\n",
       "        [0.5000]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = prob.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000],\n",
       "        [0.5000]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nested Input shape: torch.Size([1, 1, 70, 20])\n",
      "Conv2d 1 Output shape: torch.Size([1, 16, 68, 18])\n",
      "ReLU 1 Output shape: torch.Size([1, 16, 68, 18])\n",
      "MaxPool2d 1 Output shape: torch.Size([1, 16, 34, 9])\n",
      "Conv2d 2 Output shape: torch.Size([1, 32, 32, 7])\n",
      "ReLU 2 Output shape: torch.Size([1, 32, 32, 7])\n",
      "MaxPool2d 2 Output shape: torch.Size([1, 32, 16, 3])\n",
      "Conv2d 3 Output shape: torch.Size([1, 64, 14, 1])\n",
      "ReLU 3 Output shape: torch.Size([1, 64, 14, 1])\n",
      "MaxPool2d 3 Output shape: torch.Size([1, 64, 7, 1])\n",
      "Flatten Output shape: torch.Size([1, 448])\n",
      "Linear 1 Output shape: torch.Size([1, 1])\n",
      "Sigmoid Output shape: torch.Size([1, 1])\n",
      "Nested Input shape: torch.Size([1, 1, 70, 20])\n",
      "Conv2d 1 Output shape: torch.Size([1, 16, 68, 18])\n",
      "ReLU 1 Output shape: torch.Size([1, 16, 68, 18])\n",
      "MaxPool2d 1 Output shape: torch.Size([1, 16, 34, 9])\n",
      "Conv2d 2 Output shape: torch.Size([1, 32, 32, 7])\n",
      "ReLU 2 Output shape: torch.Size([1, 32, 32, 7])\n",
      "MaxPool2d 2 Output shape: torch.Size([1, 32, 16, 3])\n",
      "Conv2d 3 Output shape: torch.Size([1, 64, 14, 1])\n",
      "ReLU 3 Output shape: torch.Size([1, 64, 14, 1])\n",
      "MaxPool2d 3 Output shape: torch.Size([1, 64, 7, 1])\n",
      "Flatten Output shape: torch.Size([1, 448])\n",
      "Linear 1 Output shape: torch.Size([1, 1])\n",
      "Sigmoid Output shape: torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "output = cnn(x)  # Assuming input_tensor is the input to your model\n",
    "scalar_distance = output[0, 0].item()  # Extract the scalar distance value as a Python float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 1])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RayCastDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False, num_workers=0):\n",
    "        super().__init__(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        # Convert the list of nested arrays into a single tensor\n",
    "        return torch.tensor(batch)\n",
    "\n",
    "# Usage example\n",
    "\n",
    "# Load the combined nested arrays from HDF5 files\n",
    "combined_nested_arrays = ...  # Load the combined nested arrays\n",
    "\n",
    "# Create an instance of the RayCastDataset\n",
    "raycast_dataset = RayCastDataset(combined_nested_arrays)\n",
    "\n",
    "# Create an instance of the RayCastDataLoader\n",
    "batch_size = 32  # Choose an appropriate batch size\n",
    "shuffle = True  # Set to True if you want to shuffle the dataset\n",
    "num_workers = 0  # Set the number of workers for data loading\n",
    "raycast_dataloader = RayCastDataLoader(raycast_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "\n",
    "# Iterate over the dataloader to provide data for training\n",
    "for batch in raycast_dataloader:\n",
    "    # Perform your training steps here\n",
    "    # `batch` will contain a tensor with shape (batch_size, n, x, y)\n",
    "    # where `batch_size` is the chosen batch size, `n` is the number of nested arrays,\n",
    "    # and `x` and `y` are the dimensions of each nested array\n",
    "    # You can feed this batch into your model for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_nested_arrays = np.concatenate((nested_arrays_1, nested_arrays), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: [ 2.4456001e+03 -2.0680567e+01 -2.3686713e-01]\n",
      "Reward: 0.7999999999999999\n",
      "Done: True\n",
      "Info: {'episode_reward': 0.7999999999999999, 'episode_length': 0, 'current_observation': array([ 2.4456001e+03, -2.0680567e+01, -2.3686713e-01], dtype=float32), 'action_taken': tensor(0), 'obstacle_visible': False, 'reward_visible': True, 'goal_reached': False, 'car_collision_detected': False, 'velocity_incremented': False}\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have created an instance of UnityEnv called 'env'\n",
    "action = torch.tensor(0)  # Choose an action index within the valid range\n",
    "observation, reward, done, info = base_env.step(action)\n",
    "\n",
    "# Print the results\n",
    "print(\"Observation:\", observation)\n",
    "print(\"Reward:\", reward)\n",
    "print(\"Done:\", done)\n",
    "print(\"Info:\", info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray = base_env.RayCast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x26f22ecf0a0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJkAAAGgCAYAAAC5XsVrAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAARf0lEQVR4nO3dfUzV9d/H8ddB5HgTYHjDkUuw0y+WGWqKN4FlVsrGr7marTvNWW1NSitiLTX/0FwDtV/OGmnhH6Zrpv+kuZUaW4U250LK5aXN9CdXUEqml5egJah8rz/8cfLA8eYL5+05HJ6P7WzxOUf5nPX04/fAwbfHcRxHgKG4SG8AsY/IYI7IYI7IYI7IYI7IYI7IYI7IYI7IYI7IYM4sspUrV8rv96tHjx7Kzs7Wzp07rT4Voly8xW+6ceNGFRYWauXKlRo/frw+/PBD5efn68CBA8rIyLjqr21ubtbRo0eVmJgoj8djsT2EgeM4amhoUFpamuLirnFWOQbGjh3rFBQUBK0NGTLEmTdv3jV/bW1trSOJWye51dbWXvP/adhPsqamJlVVVWnevHlB63l5edq1a1ebxzc2NqqxsTHwsfOfN4Xco38qXt3DvT2EyQWd17f6QomJidd8bNgjO3HihC5evKjU1NSg9dTUVNXV1bV5fElJid58880QG+uueA+RRa3/vEHsei5pzC78W39yx3FCbmj+/Pk6ffp04FZbW2u1JURI2E+yfv36qVu3bm1OrePHj7c53STJ6/XK6/WGexuIImE/yRISEpSdna3y8vKg9fLycuXm5ob706ETMPkSRlFRkWbMmKHRo0crJydHZWVlqqmpUUFBgcWnQ5QzieyJJ57QyZMntXjxYh07dkxZWVn64osvNHjwYItPhyjncZzo+kGS+vp6JScna6Ie5tVlFLvgnNc3+kynT59WUlLSVR/L9y5hjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgjshgznVkO3bs0JQpU5SWliaPx6PNmzcH3e84jhYtWqS0tDT17NlTEydO1P79+8O1X3RCriM7e/asRowYodLS0pD3L1u2TMuXL1dpaakqKyvl8/k0efJkNTQ0dHiz6JxcT4nLz89Xfn5+yPscx9GKFSu0YMECTZ06VZK0du1apaamav369Zo1a1bHdotOKazXZNXV1aqrq1NeXl5gzev16r777gs55F66NOi+vr4+6IbYEtbIWkZCX++Qe+nSoPvk5OTALT09PZxbQhQweXV5vUPuJQbddwVhndzr8/kkXTrRBg4cGFi/0pB7iUH3XUFYTzK/3y+fzxc05L6pqUkVFRUMue/CXJ9kZ86c0eHDhwMfV1dXa+/evUpJSVFGRoYKCwtVXFyszMxMZWZmqri4WL169dK0adPCunF0Hq4j27Nnj+6///7Ax0VFRZKkmTNn6qOPPtLrr7+uv/76Sy+++KJOnTqlcePG6csvv1RiYmL4do1OhUH3aBcG3SOqEBnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMERnMuYqspKREY8aMUWJiogYMGKBHHnlEBw8eDHoMg+7RmqvIKioqNHv2bO3evVvl5eW6cOGC8vLydPbs2cBjGHSP1jo0wOuPP/7QgAEDVFFRoQkTJshxHKWlpamwsFBz586VdGnGeGpqqpYuXXpdg+4Z4NU53LABXqdPn5YkpaSkSGLQPUJrd2SO46ioqEj33HOPsrKyJDHoHqG1O7I5c+boxx9/1CeffNLmPgbd43LtGnT/0ksvacuWLdqxY4cGDRoUWGfQPUJxdZI5jqM5c+bo008/1VdffSW/3x90P4PuEYqrk2z27Nlav369PvvsMyUmJgaus5KTk9WzZ095PB4G3aMNV5GtWrVKkjRx4sSg9TVr1uiZZ56RJAbdow0G3aNdGHSPqEJkMEdkMEdkMEdkMEdkMNeubyt1Rf/+191t1pwr/REN8W3a217dHd4NdSKcZDBHZDBHZDBHZDDHhf91+sdrbS/cD32UHfrBnqj6dnDEcZLBHJHBHJHBHJHBHBf+HZDY58+Q693imm/wTqIbJxnMERnMERnMERnMceHfAf+VfDrkenyIC/9G681EMU4ymCMymCMymCMymCMymOPVZQfcetPJkOtxnravLg+GeFxXwUkGc0QGc0QGc0QGc1z4d8CtPf8IuR76wr+P8W6iFycZzBEZzBEZzBEZzHHh3wHbs0L/q88vHDocYrWP6V6iGScZzBEZzBEZzLmKbNWqVRo+fLiSkpKUlJSknJwcbd26NXA/88cRiqvIBg0apCVLlmjPnj3as2ePHnjgAT388MOBkJg/jlA6PFspJSVFb7/9tp577rkOzx+XYmO2UnH1d23W3vCPjcBO7NyQ2UoXL17Uhg0bdPbsWeXk5LRr/ji6BtdfJ9u3b59ycnJ07tw53XTTTdq0aZOGDh0aCCnU/PFffvnlir9fY2OjGhv//qlEBt3HHtcn2e233669e/dq9+7deuGFFzRz5kwdOHAgcL+b+eMSg+67AteRJSQk6LbbbtPo0aNVUlKiESNG6N133w2aP365q80flxh03xV0+OtkjuOosbGx3fPHvV5v4EsiLbfOrn+3pja3rszVNdkbb7yh/Px8paenq6GhQRs2bNA333yjbdu2MX8cV+Qqst9//10zZszQsWPHlJycrOHDh2vbtm2aPHmyJOaPIzRmkBtYXfNtm7XnM+6JwE7sMIMcUYX3kxlIjusW6S1EFU4ymCMymCMymCMymOPC30AvT0KktxBVOMlgjshgjshgjshgjshgjleXBrp7+LbS5TjJYI7IYI7IYI7IYI7IYI7IYI7IYI7IYI7IYI6v+Bs471yM9BaiCicZzBEZzBEZzBEZzBEZzPHq0sCfTtf+98ha4ySDOSKDOSKDOSKDOS78DZxu5ttKl+Mkgzkigzkigzkigzku/A38cZF/n+xynGQwR2QwR2Qw16HISkpKAoO7WjDsHq21O7LKykqVlZVp+PDhQesMu5dqL6S0uXVl7YrszJkzmj59ulavXq2bb745sO44jlasWKEFCxZo6tSpysrK0tq1a/Xnn39q/fr1Yds0Opd2RTZ79mw99NBDmjRpUtA6w+4Riuuvk23YsEHff/+9Kisr29zXMhrazbB7Bt3HPlcnWW1trV555RV9/PHH6tGjxxUf52bYPYPuY5+ryKqqqnT8+HFlZ2crPj5e8fHxqqio0Hvvvaf4+PjACeZm2D2D7mOfq78uH3zwQe3bty9o7dlnn9WQIUM0d+5c3XrrrYFh9yNHjpT097D7pUuXhvw9vV6vvF5vO7cfWZP+O/Qr5v9p6neDdxLdXEWWmJiorKysoLXevXurb9++gXWG3aO1sH+DnGH3aI1B9x1wpb8uu3vavv166519jHdzYzHoHlGF95N1wC9/hb7Aj/M0h1g9b7uZKMZJBnNEBnNEBnNEBnNc+HfAkTN9Q67Hx4W68K8LsdY1cJLBHJHBHJHBHJHBHJHBHK8uO+C308kh17uFeHXZn1eXgB0igzkigzkigzku/Dug4f96hb7D0/bNxv2N9xLNOMlgjshgjshgjshgjgv/6/Tvf93dZs3zv1d4cOh/9qPL4iSDOSKDOSKDOSKDOS78r9M/Xtsd6S10WpxkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMOcqskWLFsnj8QTdfD5f4H6G3CMU1yfZnXfeqWPHjgVul48mZMg9QnEdWXx8vHw+X+DWv/+lH8BnyD2uxHVkhw4dUlpamvx+v5588kkdOXJEUvuH3Dc2Nqq+vj7ohtjiKrJx48Zp3bp12r59u1avXq26ujrl5ubq5MmTVx1y33pc9OWYQR77XEWWn5+vRx99VMOGDdOkSZP0+eefS5LWrl0beIybIfcSM8i7gg59CaN3794aNmyYDh06FHiV6WbIvXTpr9SkpKSgG2JLhyJrbGzUTz/9pIEDB8rv9weG3LdoGXKfm5vb4Y2i83L1I3GvvfaapkyZooyMDB0/flxvvfWW6uvrNXPmTHk8HobcIyRXkf3666966qmndOLECfXv31933323du/ercGDB0tiyD1CY9A92oVB94gqRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzRAZzriP77bff9PTTT6tv377q1auX7rrrLlVVVQXuZ9g9WnMV2alTpzR+/Hh1795dW7du1YEDB/TOO++oT58+gccw7B6tuZoSt3TpUqWnp2vNmjWBtVtuuSXw362H3UuXpvqmpqZq/fr1mjVrVnh2jU7F1Um2ZcsWjR49Wo899pgGDBigkSNHavXq1YH72zPsnkH3sc9VZEeOHNGqVauUmZmp7du3q6CgQC+//LLWrVsnSe0ads+g+9jnKrLm5maNGjVKxcXFGjlypGbNmqXnn39eq1atCnqcm2H3DLqPfa4iGzhwoIYOHRq0dscdd6impkaS2jXsnkH3sc9VZOPHj9fBgweD1n7++efAeGiG3SMUV68uX331VeXm5qq4uFiPP/64vvvuO5WVlamsrEySGHaPkFxFNmbMGG3atEnz58/X4sWL5ff7tWLFCk2fPj3wGIbdozUG3aNdGHSPqEJkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMEdkMOdqttKN0DKF54LOS1E1kAeXu6Dzkv7+/3U1URdZQ0ODJOlbfRHhneB6NDQ0KDk5+aqPiboBXs3NzTp69KgSExPV0NCg9PR01dbWxsyw1fr6+ph4To7jqKGhQWlpaYqLu/pVV9SdZHFxcRo0aJCkv8dMx+JE31h4Ttc6wVpw4Q9zRAZzUR2Z1+vVwoUL5fV6I72VsInF53QtUXfhj9gT1ScZYgORwRyRwRyRwVxUR7Zy5Ur5/X716NFD2dnZ2rlzZ6S3dN127NihKVOmKC0tTR6PR5s3bw6633EcLVq0SGlpaerZs6cmTpyo/fv3R2azxqI2so0bN6qwsFALFizQDz/8oHvvvVf5+fmqqamJ9Nauy9mzZzVixAiVlpaGvH/ZsmVavny5SktLVVlZKZ/Pp8mTJwe+dxtTnCg1duxYp6CgIGhtyJAhzrx58yK0o/aT5GzatCnwcXNzs+Pz+ZwlS5YE1s6dO+ckJyc7H3zwQQR2aCsqT7KmpiZVVVUpLy8vaD0vL0+7du2K0K7Cp7q6WnV1dUHPz+v16r777ouJ59daVEZ24sQJXbx4UampqUHrqampqquri9CuwqflOcTq82stKiNr0fIujBaO47RZ68xi/fm1iMrI+vXrp27durX5U338+PE2f/o7I5/PJ0kx+/xai8rIEhISlJ2drfLy8qD18vJy5ebmRmhX4eP3++Xz+YKeX1NTkyoqKmLi+bUWdW9abFFUVKQZM2Zo9OjRysnJUVlZmWpqalRQUBDprV2XM2fO6PDhw4GPq6urtXfvXqWkpCgjI0OFhYUqLi5WZmamMjMzVVxcrF69emnatGkR3LWRSL+8vZr333/fGTx4sJOQkOCMGjXKqaioiPSWrtvXX3/t6NKPwgTdZs6c6TjOpS9jLFy40PH5fI7X63UmTJjg7Nu3L7KbNsJbfWAuKq/JEFuIDOaIDOaIDOaIDOaIDOaIDOaIDOaIDOaIDOaIDOaIDOb+H8QgyfQK0GWRAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(ray[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_module= NestedCNN(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the numpy array to a torch tensor\n",
    "ray_tensor = torch.from_numpy(ray)\n",
    "\n",
    "# Add a batch dimension to the tensor\n",
    "ray_tensor = ray_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 70, 20])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10, 2, 70,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nested Input shape: torch.Size([1, 1, 70, 20])\n",
      "Conv2d 1 Output shape: torch.Size([1, 16, 68, 18])\n",
      "ReLU 1 Output shape: torch.Size([1, 16, 68, 18])\n",
      "MaxPool2d 1 Output shape: torch.Size([1, 16, 34, 9])\n",
      "Conv2d 2 Output shape: torch.Size([1, 32, 32, 7])\n",
      "ReLU 2 Output shape: torch.Size([1, 32, 32, 7])\n",
      "MaxPool2d 2 Output shape: torch.Size([1, 32, 16, 3])\n",
      "Conv2d 3 Output shape: torch.Size([1, 64, 14, 1])\n",
      "ReLU 3 Output shape: torch.Size([1, 64, 14, 1])\n",
      "MaxPool2d 3 Output shape: torch.Size([1, 64, 7, 1])\n",
      "Flatten Output shape: torch.Size([1, 448])\n",
      "Linear 1 Output shape: torch.Size([1, 1])\n",
      "ReLU 4 Output shape: torch.Size([1, 1])\n",
      "Relu_Final Output shape: torch.Size([1, 1])\n",
      "Nested Input shape: torch.Size([1, 1, 70, 20])\n",
      "Conv2d 1 Output shape: torch.Size([1, 16, 68, 18])\n",
      "ReLU 1 Output shape: torch.Size([1, 16, 68, 18])\n",
      "MaxPool2d 1 Output shape: torch.Size([1, 16, 34, 9])\n",
      "Conv2d 2 Output shape: torch.Size([1, 32, 32, 7])\n",
      "ReLU 2 Output shape: torch.Size([1, 32, 32, 7])\n",
      "MaxPool2d 2 Output shape: torch.Size([1, 32, 16, 3])\n",
      "Conv2d 3 Output shape: torch.Size([1, 64, 14, 1])\n",
      "ReLU 3 Output shape: torch.Size([1, 64, 14, 1])\n",
      "MaxPool2d 3 Output shape: torch.Size([1, 64, 7, 1])\n",
      "Flatten Output shape: torch.Size([1, 448])\n",
      "Linear 1 Output shape: torch.Size([1, 1])\n",
      "ReLU 4 Output shape: torch.Size([1, 1])\n",
      "Relu_Final Output shape: torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "prob=cnn_module.forward(ray_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 1])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1376256"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5376*256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1376256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x26f2aa38f40>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(base_env.RayCast()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensordict.nn.distributions import NormalParamExtractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = len(base_env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "actor_net = nn.Sequential(\n",
    "    nn.Linear(20, 256, bias=True),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(256, 128, bias=True),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(128, num_actions, bias=True),\n",
    "    NormalParamExtractor()\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (3): Tanh()\n",
       "  (4): Linear(in_features=128, out_features=15, bias=True)\n",
       "  (5): NormalParamExtractor()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensordict.nn import TensorDictModule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_keys = ['' ,'' ]\n",
    "policy_module = TensorDictModule(actor_net, in_keys=in_keys, out_keys=[\"loc\", \"scale\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDictModule(\n",
       "    module=Sequential(\n",
       "      (0): Linear(in_features=20, out_features=256, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (3): Tanh()\n",
       "      (4): Linear(in_features=128, out_features=15, bias=True)\n",
       "      (5): NormalParamExtractor()\n",
       "    ),\n",
       "    device=cuda:0,\n",
       "    in_keys=['', ''],\n",
       "    out_keys=['loc', 'scale'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.modules import ProbabilisticActor, TanhNormal, ValueOperator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_module = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=base_env.action_space,\n",
    "    in_keys=['' ,'' ],\n",
    "    distribution_class=TanhNormal,\n",
    "    distribution_kwargs={\n",
    "        \"num_actions\": num_actions,\n",
    "    },\n",
    "    return_log_prob=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.modules import ProbabilisticActor, TanhNormal, ValueOperator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_module = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=base_env.action_space,\n",
    "    in_keys=['' ,'' ],\n",
    "    distribution_class=TanhNormal,\n",
    "    distribution_kwargs={\n",
    "        \"num_actions\": num_actions,\n",
    "    },\n",
    "    return_log_prob=True,\n",
    ")\n",
    "\n",
    "\n",
    "demo_params = {\n",
    "    \"module.param1\": base_env.RayCast()[0],\n",
    "    \"module.param2\": base_env.RayCast()[1],\n",
    "    # Add more parameter-value pairs as needed\n",
    "}\n",
    "\n",
    "\n",
    "# Define the minimum and maximum values for the action space\n",
    "action_space_min = torch.tensor([-1.0])\n",
    "action_space_max = torch.tensor([1.0])\n",
    "\n",
    "# Set the parameters for each module in the ModuleList\n",
    "for key, value in demo_params.items():\n",
    "    module_key, param_name = key.split(\".\", 1)\n",
    "    module = getattr(policy_module, module_key)\n",
    "    setattr(module, param_name, value)\n",
    "\n",
    "policy_module = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=base_env.action_space,\n",
    "    in_keys=[\"loc\", \"scale\"],\n",
    "    distribution_class=TanhNormal,\n",
    "    distribution_kwargs={\n",
    "        \"min\": action_space_min,\n",
    "        \"max\": action_space_max,\n",
    "    },\n",
    "    return_log_prob=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\RL_UNITY\\full_car_envs\\my_experiment\\Assets\\subAssets\\Python\\environment\\Env5000\\TORCH_RL.ipynb Cell 34\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/TORCH_RL.ipynb#Y130sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Forward pass through the policy module\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/TORCH_RL.ipynb#Y130sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/TORCH_RL.ipynb#Y130sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     output \u001b[39m=\u001b[39m policy_module({raycast})\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/TORCH_RL.ipynb#Y130sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Extract the action from the output\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/TORCH_RL.ipynb#Y130sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m action \u001b[39m=\u001b[39m output[\u001b[39m'\u001b[39m\u001b[39maction\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:431\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    430\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 431\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m), fun_name)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    432\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    433\u001b[0m         pattern \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.*takes \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ positional arguments but \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ were given|got multiple values for argument\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\common.py:266\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[1;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m    262\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[0;32m    263\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe key \u001b[39m\u001b[39m{\u001b[39;00mexpected_key\u001b[39m}\u001b[39;00m\u001b[39m wasn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt found in the keyword arguments \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    264\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut is expected to execute that function.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    265\u001b[0m             )\n\u001b[1;32m--> 266\u001b[0m tensordict \u001b[39m=\u001b[39m make_tensordict(\n\u001b[0;32m    267\u001b[0m     tensordict_values,\n\u001b[0;32m    268\u001b[0m     batch_size\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mSize([]) \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauto_batch_size \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    269\u001b[0m )\n\u001b[0;32m    270\u001b[0m out \u001b[39m=\u001b[39m func(_self, tensordict, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    271\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(out[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m dest)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\tensordict.py:7150\u001b[0m, in \u001b[0;36mmake_tensordict\u001b[1;34m(input_dict, batch_size, device, **kwargs)\u001b[0m\n\u001b[0;32m   7148\u001b[0m \u001b[39mif\u001b[39;00m input_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   7149\u001b[0m     kwargs\u001b[39m.\u001b[39mupdate(input_dict)\n\u001b[1;32m-> 7150\u001b[0m \u001b[39mreturn\u001b[39;00m TensorDict\u001b[39m.\u001b[39;49mfrom_dict(kwargs, batch_size\u001b[39m=\u001b[39;49mbatch_size, device\u001b[39m=\u001b[39;49mdevice)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\tensordict.py:3464\u001b[0m, in \u001b[0;36mTensorDict.from_dict\u001b[1;34m(cls, input_dict, batch_size, device)\u001b[0m\n\u001b[0;32m   3462\u001b[0m         input_dict[key] \u001b[39m=\u001b[39m TensorDict(value, batch_size_set, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m   3463\u001b[0m \u001b[39m# _run_checks=False breaks because a tensor may have the same batch-size as the tensordict\u001b[39;00m\n\u001b[1;32m-> 3464\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\n\u001b[0;32m   3465\u001b[0m     input_dict,\n\u001b[0;32m   3466\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size_set,\n\u001b[0;32m   3467\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m   3468\u001b[0m )\n\u001b[0;32m   3469\u001b[0m \u001b[39mif\u001b[39;00m batch_size \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3470\u001b[0m     _set_max_batch_size(out)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\tensordict.py:3399\u001b[0m, in \u001b[0;36mTensorDict.__init__\u001b[1;34m(self, source, batch_size, device, names, _run_checks, _is_shared, _is_memmap)\u001b[0m\n\u001b[0;32m   3397\u001b[0m \u001b[39mif\u001b[39;00m source \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3398\u001b[0m     \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m source\u001b[39m.\u001b[39mitems():\n\u001b[1;32m-> 3399\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset(key, value)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\tensordict.py:3689\u001b[0m, in \u001b[0;36mTensorDict.set\u001b[1;34m(self, key, value, inplace)\u001b[0m\n\u001b[0;32m   3686\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_locked \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m inplace:\n\u001b[0;32m   3687\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(TensorDictBase\u001b[39m.\u001b[39mLOCK_ERROR)\n\u001b[1;32m-> 3689\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_value(value)\n\u001b[0;32m   3690\u001b[0m \u001b[39m# not calling set_ to avoid re-validate key\u001b[39;00m\n\u001b[0;32m   3691\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set(key, value, inplace\u001b[39m=\u001b[39minplace)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\tensordict.py:1538\u001b[0m, in \u001b[0;36mTensorDictBase._validate_value\u001b[1;34m(self, value, check_shape)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1537\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1538\u001b[0m         value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_to_tensor(value)\n\u001b[0;32m   1539\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m   1540\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1541\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwe only supports tensorclasses, tensordicts,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1542\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m numeric scalars and tensors. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1543\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\tensordict.py:1508\u001b[0m, in \u001b[0;36mTensorDictBase._convert_to_tensor\u001b[1;34m(self, array)\u001b[0m\n\u001b[0;32m   1506\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(array, np\u001b[39m.\u001b[39mbool_):\n\u001b[0;32m   1507\u001b[0m     array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mitem()\n\u001b[1;32m-> 1508\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mas_tensor(array, device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Could not infer dtype of set"
     ]
    }
   ],
   "source": [
    "# Forward pass through the policy module\n",
    "with torch.no_grad():\n",
    "    output = policy_module({raycast})\n",
    "\n",
    "# Extract the action from the output\n",
    "action = output['action'].squeeze().item()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ModuleList' object has no attribute 'param_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\common.py:837\u001b[0m, in \u001b[0;36mTensorDictModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 837\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__getattr__\u001b[39;49m(name)\n\u001b[0;32m    838\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m \u001b[39mas\u001b[39;00m err1:\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ProbabilisticActor' object has no attribute 'param_name'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\RL_UNITY\\full_car_envs\\my_experiment\\Assets\\subAssets\\Python\\environment\\Env5000\\TORCH_RL.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/TORCH_RL.ipynb#Y114sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m2\u001b[39m, \u001b[39m20\u001b[39m, \u001b[39m70\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/TORCH_RL.ipynb#Y114sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m params \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mparam_name\u001b[39m\u001b[39m'\u001b[39m: demo_params}  \u001b[39m# Replace with actual parameters\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/TORCH_RL.ipynb#Y114sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m policy_module(x, params\u001b[39m=\u001b[39;49mparams)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:415\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    413\u001b[0m     args \u001b[39m=\u001b[39m args[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    414\u001b[0m     \u001b[39m# get the previous params, and tell the submodules not to look for params anymore\u001b[39;00m\n\u001b[1;32m--> 415\u001b[0m old_params \u001b[39m=\u001b[39m _assign_params(\n\u001b[0;32m    416\u001b[0m     \u001b[39mself\u001b[39;49m, params, make_stateless\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, return_old_tensordict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m    417\u001b[0m )\n\u001b[0;32m    418\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    419\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m), fun_name)(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:492\u001b[0m, in \u001b[0;36m_assign_params\u001b[1;34m(module, params, make_stateless, return_old_tensordict)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_assign_params\u001b[39m(\n\u001b[0;32m    486\u001b[0m     module: nn\u001b[39m.\u001b[39mModule,\n\u001b[0;32m    487\u001b[0m     params: TensorDict,\n\u001b[0;32m    488\u001b[0m     make_stateless: \u001b[39mbool\u001b[39m,\n\u001b[0;32m    489\u001b[0m     return_old_tensordict: \u001b[39mbool\u001b[39m,\n\u001b[0;32m    490\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m TensorDict \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    491\u001b[0m     \u001b[39mif\u001b[39;00m params \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 492\u001b[0m         \u001b[39mreturn\u001b[39;00m _swap_state(module, params, make_stateless, return_old_tensordict)\n\u001b[0;32m    494\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:314\u001b[0m, in \u001b[0;36m_swap_state\u001b[1;34m(model, tensordict, is_stateless, return_old_tensordict, old_tensordict)\u001b[0m\n\u001b[0;32m    312\u001b[0m is_param \u001b[39m=\u001b[39m key \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    313\u001b[0m \u001b[39mif\u001b[39;00m return_old_tensordict:\n\u001b[1;32m--> 314\u001b[0m     old_attr \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(model, key)\n\u001b[0;32m    315\u001b[0m     \u001b[39mif\u001b[39;00m old_attr \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m         old_attr \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39m*\u001b[39mvalue\u001b[39m.\u001b[39mshape, \u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\common.py:842\u001b[0m, in \u001b[0;36mTensorDictModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    840\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getattr__\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmodule\u001b[39m\u001b[39m\"\u001b[39m), name)\n\u001b[0;32m    841\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err2:\n\u001b[1;32m--> 842\u001b[0m     \u001b[39mraise\u001b[39;00m err2 \u001b[39mfrom\u001b[39;00m \u001b[39merr1\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\common.py:840\u001b[0m, in \u001b[0;36mTensorDictModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    838\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m \u001b[39mas\u001b[39;00m err1:\n\u001b[0;32m    839\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 840\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__getattr__\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mmodule\u001b[39;49m\u001b[39m\"\u001b[39;49m), name)\n\u001b[0;32m    841\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err2:\n\u001b[0;32m    842\u001b[0m         \u001b[39mraise\u001b[39;00m err2 \u001b[39mfrom\u001b[39;00m \u001b[39merr1\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ModuleList' object has no attribute 'param_name'"
     ]
    }
   ],
   "source": [
    "# Test the policy module\n",
    "x = torch.randn(2, 20, 70).to(device)\n",
    "params = {'param_name': demo_params}  # Replace with actual parameters\n",
    "policy_module(x, params=params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 20, 70).to(device)\n",
    "params = {'param_name': demo_params}  # Replace with actual parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'param_name': {'module.param1': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       "  'module.param2': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ModuleList' object has no attribute 'param_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\common.py:837\u001b[0m, in \u001b[0;36mTensorDictModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 837\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__getattr__\u001b[39;49m(name)\n\u001b[0;32m    838\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m \u001b[39mas\u001b[39;00m err1:\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ProbabilisticActor' object has no attribute 'param_name'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\RL_UNITY\\full_car_envs\\my_experiment\\Assets\\subAssets\\Python\\environment\\Env5000\\TORCH_RL.ipynb Cell 29\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/TORCH_RL.ipynb#Y111sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Test the policy module\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/TORCH_RL.ipynb#Y111sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m policy_module(x, params\u001b[39m=\u001b[39;49mparams)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:415\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    413\u001b[0m     args \u001b[39m=\u001b[39m args[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    414\u001b[0m     \u001b[39m# get the previous params, and tell the submodules not to look for params anymore\u001b[39;00m\n\u001b[1;32m--> 415\u001b[0m old_params \u001b[39m=\u001b[39m _assign_params(\n\u001b[0;32m    416\u001b[0m     \u001b[39mself\u001b[39;49m, params, make_stateless\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, return_old_tensordict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m    417\u001b[0m )\n\u001b[0;32m    418\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    419\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m), fun_name)(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:492\u001b[0m, in \u001b[0;36m_assign_params\u001b[1;34m(module, params, make_stateless, return_old_tensordict)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_assign_params\u001b[39m(\n\u001b[0;32m    486\u001b[0m     module: nn\u001b[39m.\u001b[39mModule,\n\u001b[0;32m    487\u001b[0m     params: TensorDict,\n\u001b[0;32m    488\u001b[0m     make_stateless: \u001b[39mbool\u001b[39m,\n\u001b[0;32m    489\u001b[0m     return_old_tensordict: \u001b[39mbool\u001b[39m,\n\u001b[0;32m    490\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m TensorDict \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    491\u001b[0m     \u001b[39mif\u001b[39;00m params \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 492\u001b[0m         \u001b[39mreturn\u001b[39;00m _swap_state(module, params, make_stateless, return_old_tensordict)\n\u001b[0;32m    494\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:314\u001b[0m, in \u001b[0;36m_swap_state\u001b[1;34m(model, tensordict, is_stateless, return_old_tensordict, old_tensordict)\u001b[0m\n\u001b[0;32m    312\u001b[0m is_param \u001b[39m=\u001b[39m key \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    313\u001b[0m \u001b[39mif\u001b[39;00m return_old_tensordict:\n\u001b[1;32m--> 314\u001b[0m     old_attr \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(model, key)\n\u001b[0;32m    315\u001b[0m     \u001b[39mif\u001b[39;00m old_attr \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m         old_attr \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39m*\u001b[39mvalue\u001b[39m.\u001b[39mshape, \u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\common.py:842\u001b[0m, in \u001b[0;36mTensorDictModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    840\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getattr__\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmodule\u001b[39m\u001b[39m\"\u001b[39m), name)\n\u001b[0;32m    841\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err2:\n\u001b[1;32m--> 842\u001b[0m     \u001b[39mraise\u001b[39;00m err2 \u001b[39mfrom\u001b[39;00m \u001b[39merr1\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\common.py:840\u001b[0m, in \u001b[0;36mTensorDictModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    838\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m \u001b[39mas\u001b[39;00m err1:\n\u001b[0;32m    839\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 840\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__getattr__\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mmodule\u001b[39;49m\u001b[39m\"\u001b[39;49m), name)\n\u001b[0;32m    841\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err2:\n\u001b[0;32m    842\u001b[0m         \u001b[39mraise\u001b[39;00m err2 \u001b[39mfrom\u001b[39;00m \u001b[39merr1\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ModuleList' object has no attribute 'param_name'"
     ]
    }
   ],
   "source": [
    "# Test the policy module\n",
    "\n",
    "policy_module(x, params=params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "It seems you tried to provide the parameters as an argument to the module when the module was not stateless. If this is the case, this error should vanish by providing the parameters using the ``module(..., params=params)`` syntax.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:431\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 431\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m), fun_name)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    432\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\common.py:273\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[1;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[39mreturn\u001b[39;00m out[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m out\n\u001b[1;32m--> 273\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\_contextlib.py:126\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\utils.py:253\u001b[0m, in \u001b[0;36mset_skip_existing.__call__.<locals>.wrapper\u001b[1;34m(_self, tensordict, *args, **kwargs)\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[39mreturn\u001b[39;00m tensordict\n\u001b[1;32m--> 253\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\common.py:810\u001b[0m, in \u001b[0;36mTensorDictModule.forward\u001b[1;34m(self, tensordict, tensordict_out, *args, **kwargs)\u001b[0m\n\u001b[0;32m    809\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 810\u001b[0m         \u001b[39mraise\u001b[39;00m err\n\u001b[0;32m    811\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, (\u001b[39mdict\u001b[39m, TensorDictBase)):\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\common.py:798\u001b[0m, in \u001b[0;36mTensorDictModule.forward\u001b[1;34m(self, tensordict, tensordict_out, *args, **kwargs)\u001b[0m\n\u001b[0;32m    797\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 798\u001b[0m     tensors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_module(tensors, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    799\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\common.py:766\u001b[0m, in \u001b[0;36mTensorDictModule._call_module\u001b[1;34m(self, tensors, **kwargs)\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_module\u001b[39m(\n\u001b[0;32m    764\u001b[0m     \u001b[39mself\u001b[39m, tensors: Sequence[Tensor], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[0;32m    765\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor \u001b[39m|\u001b[39m Sequence[Tensor]:\n\u001b[1;32m--> 766\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule(\u001b[39m*\u001b[39;49mtensors, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    767\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:443\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 443\u001b[0m     \u001b[39mraise\u001b[39;00m err\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:431\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 431\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m), fun_name)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    432\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\RL_UNITY\\full_car_envs\\my_experiment\\Assets\\subAssets\\Python\\environment\\Env5000\\TORCH_RL.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/TORCH_RL.ipynb#Y110sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#test the policy module\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/TORCH_RL.ipynb#Y110sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m20\u001b[39m,\u001b[39m70\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/TORCH_RL.ipynb#Y110sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m policy_module(x)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:443\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    438\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt seems you tried to provide the parameters as an argument to the module when the module was not stateless. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    439\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf this is the case, this error should vanish by providing the parameters using the ``module(..., params=params)`` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    440\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msyntax.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 443\u001b[0m     \u001b[39mraise\u001b[39;00m err\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:431\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    430\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 431\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m), fun_name)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    432\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    433\u001b[0m         pattern \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.*takes \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ positional arguments but \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ were given|got multiple values for argument\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\common.py:270\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[1;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[0;32m    263\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe key \u001b[39m\u001b[39m{\u001b[39;00mexpected_key\u001b[39m}\u001b[39;00m\u001b[39m wasn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt found in the keyword arguments \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    264\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut is expected to execute that function.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    265\u001b[0m             )\n\u001b[0;32m    266\u001b[0m tensordict \u001b[39m=\u001b[39m make_tensordict(\n\u001b[0;32m    267\u001b[0m     tensordict_values,\n\u001b[0;32m    268\u001b[0m     batch_size\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mSize([]) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_batch_size \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    269\u001b[0m )\n\u001b[1;32m--> 270\u001b[0m out \u001b[39m=\u001b[39m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    271\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(out[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m dest)\n\u001b[0;32m    272\u001b[0m \u001b[39mreturn\u001b[39;00m out[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\_contextlib.py:126\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    125\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 126\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\utils.py:253\u001b[0m, in \u001b[0;36mset_skip_existing.__call__.<locals>.wrapper\u001b[1;34m(_self, tensordict, *args, **kwargs)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    248\u001b[0m     skip_existing()\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(key \u001b[39min\u001b[39;00m tensordict\u001b[39m.\u001b[39mkeys(\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m out_keys)\n\u001b[0;32m    250\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(key \u001b[39min\u001b[39;00m out_keys \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m in_keys)\n\u001b[0;32m    251\u001b[0m ):\n\u001b[0;32m    252\u001b[0m     \u001b[39mreturn\u001b[39;00m tensordict\n\u001b[1;32m--> 253\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\probabilistic.py:511\u001b[0m, in \u001b[0;36mProbabilisticTensorDictSequential.forward\u001b[1;34m(self, tensordict, tensordict_out, **kwargs)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[39m@dispatch\u001b[39m(auto_batch_size\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    504\u001b[0m \u001b[39m@set_skip_existing\u001b[39m(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    505\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    510\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m TensorDictBase:\n\u001b[1;32m--> 511\u001b[0m     tensordict_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_dist_params(tensordict, tensordict_out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    512\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m](tensordict_out, _requires_sample\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_requires_sample)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\probabilistic.py:487\u001b[0m, in \u001b[0;36mProbabilisticTensorDictSequential.get_dist_params\u001b[1;34m(self, tensordict, tensordict_out, **kwargs)\u001b[0m\n\u001b[0;32m    485\u001b[0m     \u001b[39mtype\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdefault_interaction_type\n\u001b[0;32m    486\u001b[0m \u001b[39mwith\u001b[39;00m set_interaction_type(\u001b[39mtype\u001b[39m):\n\u001b[1;32m--> 487\u001b[0m     \u001b[39mreturn\u001b[39;00m tds(tensordict, tensordict_out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:443\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    438\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt seems you tried to provide the parameters as an argument to the module when the module was not stateless. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    439\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf this is the case, this error should vanish by providing the parameters using the ``module(..., params=params)`` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    440\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msyntax.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 443\u001b[0m     \u001b[39mraise\u001b[39;00m err\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:431\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    430\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 431\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m), fun_name)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    432\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    433\u001b[0m         pattern \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.*takes \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ positional arguments but \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ were given|got multiple values for argument\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\common.py:273\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[1;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(out[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m dest)\n\u001b[0;32m    272\u001b[0m     \u001b[39mreturn\u001b[39;00m out[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m out\n\u001b[1;32m--> 273\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\_contextlib.py:126\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    125\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 126\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\utils.py:253\u001b[0m, in \u001b[0;36mset_skip_existing.__call__.<locals>.wrapper\u001b[1;34m(_self, tensordict, *args, **kwargs)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    248\u001b[0m     skip_existing()\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(key \u001b[39min\u001b[39;00m tensordict\u001b[39m.\u001b[39mkeys(\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m out_keys)\n\u001b[0;32m    250\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(key \u001b[39min\u001b[39;00m out_keys \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m in_keys)\n\u001b[0;32m    251\u001b[0m ):\n\u001b[0;32m    252\u001b[0m     \u001b[39mreturn\u001b[39;00m tensordict\n\u001b[1;32m--> 253\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\sequence.py:427\u001b[0m, in \u001b[0;36mTensorDictSequential.forward\u001b[1;34m(self, tensordict, tensordict_out, **kwargs)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(kwargs):\n\u001b[0;32m    426\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule:\n\u001b[1;32m--> 427\u001b[0m         tensordict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_module(module, tensordict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    428\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    429\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    430\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTensorDictSequential does not support keyword arguments other than \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensordict_out\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or in_keys: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_keys\u001b[39m}\u001b[39;00m\u001b[39m. Got \u001b[39m\u001b[39m{\u001b[39;00mkwargs\u001b[39m.\u001b[39mkeys()\u001b[39m}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    431\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\sequence.py:407\u001b[0m, in \u001b[0;36mTensorDictSequential._run_module\u001b[1;34m(self, module, tensordict, **kwargs)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_module\u001b[39m(\n\u001b[0;32m    399\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    400\u001b[0m     module: TensorDictModule,\n\u001b[0;32m    401\u001b[0m     tensordict: TensorDictBase,\n\u001b[0;32m    402\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    403\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    404\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartial_tolerant \u001b[39mor\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[0;32m    405\u001b[0m         key \u001b[39min\u001b[39;00m tensordict\u001b[39m.\u001b[39mkeys(include_nested\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39min_keys\n\u001b[0;32m    406\u001b[0m     ):\n\u001b[1;32m--> 407\u001b[0m         tensordict \u001b[39m=\u001b[39m module(tensordict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    408\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartial_tolerant \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(tensordict, LazyStackedTensorDict):\n\u001b[0;32m    409\u001b[0m         \u001b[39mfor\u001b[39;00m sub_td \u001b[39min\u001b[39;00m tensordict\u001b[39m.\u001b[39mtensordicts:\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:443\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    438\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt seems you tried to provide the parameters as an argument to the module when the module was not stateless. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    439\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf this is the case, this error should vanish by providing the parameters using the ``module(..., params=params)`` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    440\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msyntax.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 443\u001b[0m     \u001b[39mraise\u001b[39;00m err\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:431\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    430\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 431\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m), fun_name)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    432\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    433\u001b[0m         pattern \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.*takes \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ positional arguments but \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ were given|got multiple values for argument\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\common.py:273\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[1;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(out[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m dest)\n\u001b[0;32m    272\u001b[0m     \u001b[39mreturn\u001b[39;00m out[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m out\n\u001b[1;32m--> 273\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\_contextlib.py:126\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    125\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 126\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\utils.py:253\u001b[0m, in \u001b[0;36mset_skip_existing.__call__.<locals>.wrapper\u001b[1;34m(_self, tensordict, *args, **kwargs)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    248\u001b[0m     skip_existing()\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(key \u001b[39min\u001b[39;00m tensordict\u001b[39m.\u001b[39mkeys(\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m out_keys)\n\u001b[0;32m    250\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(key \u001b[39min\u001b[39;00m out_keys \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m in_keys)\n\u001b[0;32m    251\u001b[0m ):\n\u001b[0;32m    252\u001b[0m     \u001b[39mreturn\u001b[39;00m tensordict\n\u001b[1;32m--> 253\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\probabilistic.py:511\u001b[0m, in \u001b[0;36mProbabilisticTensorDictSequential.forward\u001b[1;34m(self, tensordict, tensordict_out, **kwargs)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[39m@dispatch\u001b[39m(auto_batch_size\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    504\u001b[0m \u001b[39m@set_skip_existing\u001b[39m(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    505\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    510\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m TensorDictBase:\n\u001b[1;32m--> 511\u001b[0m     tensordict_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_dist_params(tensordict, tensordict_out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    512\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m](tensordict_out, _requires_sample\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_requires_sample)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\probabilistic.py:487\u001b[0m, in \u001b[0;36mProbabilisticTensorDictSequential.get_dist_params\u001b[1;34m(self, tensordict, tensordict_out, **kwargs)\u001b[0m\n\u001b[0;32m    485\u001b[0m     \u001b[39mtype\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdefault_interaction_type\n\u001b[0;32m    486\u001b[0m \u001b[39mwith\u001b[39;00m set_interaction_type(\u001b[39mtype\u001b[39m):\n\u001b[1;32m--> 487\u001b[0m     \u001b[39mreturn\u001b[39;00m tds(tensordict, tensordict_out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:443\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    438\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt seems you tried to provide the parameters as an argument to the module when the module was not stateless. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    439\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf this is the case, this error should vanish by providing the parameters using the ``module(..., params=params)`` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    440\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msyntax.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 443\u001b[0m     \u001b[39mraise\u001b[39;00m err\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:431\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    430\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 431\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m), fun_name)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    432\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    433\u001b[0m         pattern \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.*takes \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ positional arguments but \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ were given|got multiple values for argument\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\common.py:273\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[1;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(out[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m dest)\n\u001b[0;32m    272\u001b[0m     \u001b[39mreturn\u001b[39;00m out[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m out\n\u001b[1;32m--> 273\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\_contextlib.py:126\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    125\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 126\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\utils.py:253\u001b[0m, in \u001b[0;36mset_skip_existing.__call__.<locals>.wrapper\u001b[1;34m(_self, tensordict, *args, **kwargs)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    248\u001b[0m     skip_existing()\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(key \u001b[39min\u001b[39;00m tensordict\u001b[39m.\u001b[39mkeys(\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m out_keys)\n\u001b[0;32m    250\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(key \u001b[39min\u001b[39;00m out_keys \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m in_keys)\n\u001b[0;32m    251\u001b[0m ):\n\u001b[0;32m    252\u001b[0m     \u001b[39mreturn\u001b[39;00m tensordict\n\u001b[1;32m--> 253\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\sequence.py:427\u001b[0m, in \u001b[0;36mTensorDictSequential.forward\u001b[1;34m(self, tensordict, tensordict_out, **kwargs)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(kwargs):\n\u001b[0;32m    426\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule:\n\u001b[1;32m--> 427\u001b[0m         tensordict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_module(module, tensordict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    428\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    429\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    430\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTensorDictSequential does not support keyword arguments other than \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensordict_out\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or in_keys: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_keys\u001b[39m}\u001b[39;00m\u001b[39m. Got \u001b[39m\u001b[39m{\u001b[39;00mkwargs\u001b[39m.\u001b[39mkeys()\u001b[39m}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    431\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\sequence.py:407\u001b[0m, in \u001b[0;36mTensorDictSequential._run_module\u001b[1;34m(self, module, tensordict, **kwargs)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_module\u001b[39m(\n\u001b[0;32m    399\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    400\u001b[0m     module: TensorDictModule,\n\u001b[0;32m    401\u001b[0m     tensordict: TensorDictBase,\n\u001b[0;32m    402\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    403\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    404\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartial_tolerant \u001b[39mor\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[0;32m    405\u001b[0m         key \u001b[39min\u001b[39;00m tensordict\u001b[39m.\u001b[39mkeys(include_nested\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39min_keys\n\u001b[0;32m    406\u001b[0m     ):\n\u001b[1;32m--> 407\u001b[0m         tensordict \u001b[39m=\u001b[39m module(tensordict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    408\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartial_tolerant \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(tensordict, LazyStackedTensorDict):\n\u001b[0;32m    409\u001b[0m         \u001b[39mfor\u001b[39;00m sub_td \u001b[39min\u001b[39;00m tensordict\u001b[39m.\u001b[39mtensordicts:\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:437\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    434\u001b[0m pattern \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mcompile(pattern)\n\u001b[0;32m    435\u001b[0m \u001b[39mif\u001b[39;00m pattern\u001b[39m.\u001b[39msearch(\u001b[39mstr\u001b[39m(err)) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(args[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], TensorDictBase):\n\u001b[0;32m    436\u001b[0m     \u001b[39m# this is raised whenever the module is an nn.Module (not a TensorDictModuleBase)\u001b[39;00m\n\u001b[1;32m--> 437\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    438\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt seems you tried to provide the parameters as an argument to the module when the module was not stateless. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    439\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf this is the case, this error should vanish by providing the parameters using the ``module(..., params=params)`` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    440\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msyntax.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    443\u001b[0m     \u001b[39mraise\u001b[39;00m err\n",
      "\u001b[1;31mTypeError\u001b[0m: It seems you tried to provide the parameters as an argument to the module when the module was not stateless. If this is the case, this error should vanish by providing the parameters using the ``module(..., params=params)`` syntax."
     ]
    }
   ],
   "source": [
    "#test the policy module\n",
    "x = torch.randn(20,70).to(device)\n",
    "policy_module(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProbabilisticActor(\n",
       "    module=ModuleList(\n",
       "      (0): ProbabilisticActor(\n",
       "          module=ModuleList(\n",
       "            (0): TensorDictModule(\n",
       "                module=Sequential(\n",
       "                  (0): Linear(in_features=20, out_features=256, bias=True)\n",
       "                  (1): Tanh()\n",
       "                  (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "                  (3): Tanh()\n",
       "                  (4): Linear(in_features=128, out_features=15, bias=True)\n",
       "                  (5): NormalParamExtractor()\n",
       "                ),\n",
       "                device=cuda:0,\n",
       "                in_keys=['', ''],\n",
       "                out_keys=['loc', 'scale'])\n",
       "            (1): SafeProbabilisticModule()\n",
       "          ),\n",
       "          device=cuda:0,\n",
       "          in_keys=[''],\n",
       "          out_keys=['loc', 'scale', 'action', 'sample_log_prob'])\n",
       "      (1): SafeProbabilisticModule()\n",
       "    ),\n",
       "    device=cuda:0,\n",
       "    in_keys=[''],\n",
       "    out_keys=['loc', 'scale', 'action', 'sample_log_prob'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cells=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_net = nn.Sequential(\n",
    "    nn.Linear(20, 256, bias=True),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(256, 128, bias=True),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(128, 1, bias=True),\n",
    "    NormalParamExtractor()\n",
    ").to(device)\n",
    "\n",
    "value_module = ValueOperator(\n",
    "    module=value_net,\n",
    "    in_keys= ['' ,'' ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ValueOperator(\n",
       "    module=Sequential(\n",
       "      (0): Linear(in_features=20, out_features=256, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (3): Tanh()\n",
       "      (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "      (5): NormalParamExtractor()\n",
       "    ),\n",
       "    device=cuda:0,\n",
       "    in_keys=['', ''],\n",
       "    out_keys=['state_value'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (3): Tanh()\n",
       "  (4): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (5): NormalParamExtractor()\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_env.RayCast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "It seems you tried to provide the parameters as an argument to the module when the module was not stateless. If this is the case, this error should vanish by providing the parameters using the ``module(..., params=params)`` syntax.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:431\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 431\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m), fun_name)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    432\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\common.py:273\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[1;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[39mreturn\u001b[39;00m out[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m out\n\u001b[1;32m--> 273\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\_contextlib.py:126\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\utils.py:253\u001b[0m, in \u001b[0;36mset_skip_existing.__call__.<locals>.wrapper\u001b[1;34m(_self, tensordict, *args, **kwargs)\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[39mreturn\u001b[39;00m tensordict\n\u001b[1;32m--> 253\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\common.py:810\u001b[0m, in \u001b[0;36mTensorDictModule.forward\u001b[1;34m(self, tensordict, tensordict_out, *args, **kwargs)\u001b[0m\n\u001b[0;32m    809\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 810\u001b[0m         \u001b[39mraise\u001b[39;00m err\n\u001b[0;32m    811\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, (\u001b[39mdict\u001b[39m, TensorDictBase)):\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\common.py:798\u001b[0m, in \u001b[0;36mTensorDictModule.forward\u001b[1;34m(self, tensordict, tensordict_out, *args, **kwargs)\u001b[0m\n\u001b[0;32m    797\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 798\u001b[0m     tensors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_module(tensors, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    799\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\common.py:766\u001b[0m, in \u001b[0;36mTensorDictModule._call_module\u001b[1;34m(self, tensors, **kwargs)\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_module\u001b[39m(\n\u001b[0;32m    764\u001b[0m     \u001b[39mself\u001b[39m, tensors: Sequence[Tensor], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[0;32m    765\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor \u001b[39m|\u001b[39m Sequence[Tensor]:\n\u001b[1;32m--> 766\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule(\u001b[39m*\u001b[39;49mtensors, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    767\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:443\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 443\u001b[0m     \u001b[39mraise\u001b[39;00m err\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:431\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 431\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m), fun_name)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    432\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\RL_UNITY\\full_car_envs\\my_experiment\\Assets\\subAssets\\Python\\environment\\Env5000\\TORCH_RL.ipynb Cell 33\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/TORCH_RL.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRunning policy:\u001b[39m\u001b[39m\"\u001b[39m, policy_module(base_env\u001b[39m.\u001b[39;49mRayCast()))\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:443\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    438\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt seems you tried to provide the parameters as an argument to the module when the module was not stateless. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    439\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf this is the case, this error should vanish by providing the parameters using the ``module(..., params=params)`` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    440\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msyntax.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 443\u001b[0m     \u001b[39mraise\u001b[39;00m err\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:431\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    430\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 431\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m), fun_name)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    432\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    433\u001b[0m         pattern \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.*takes \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ positional arguments but \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ were given|got multiple values for argument\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\common.py:270\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[1;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[0;32m    263\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe key \u001b[39m\u001b[39m{\u001b[39;00mexpected_key\u001b[39m}\u001b[39;00m\u001b[39m wasn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt found in the keyword arguments \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    264\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut is expected to execute that function.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    265\u001b[0m             )\n\u001b[0;32m    266\u001b[0m tensordict \u001b[39m=\u001b[39m make_tensordict(\n\u001b[0;32m    267\u001b[0m     tensordict_values,\n\u001b[0;32m    268\u001b[0m     batch_size\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mSize([]) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_batch_size \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    269\u001b[0m )\n\u001b[1;32m--> 270\u001b[0m out \u001b[39m=\u001b[39m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    271\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(out[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m dest)\n\u001b[0;32m    272\u001b[0m \u001b[39mreturn\u001b[39;00m out[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\_contextlib.py:126\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    125\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 126\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\utils.py:253\u001b[0m, in \u001b[0;36mset_skip_existing.__call__.<locals>.wrapper\u001b[1;34m(_self, tensordict, *args, **kwargs)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    248\u001b[0m     skip_existing()\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(key \u001b[39min\u001b[39;00m tensordict\u001b[39m.\u001b[39mkeys(\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m out_keys)\n\u001b[0;32m    250\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(key \u001b[39min\u001b[39;00m out_keys \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m in_keys)\n\u001b[0;32m    251\u001b[0m ):\n\u001b[0;32m    252\u001b[0m     \u001b[39mreturn\u001b[39;00m tensordict\n\u001b[1;32m--> 253\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\probabilistic.py:511\u001b[0m, in \u001b[0;36mProbabilisticTensorDictSequential.forward\u001b[1;34m(self, tensordict, tensordict_out, **kwargs)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[39m@dispatch\u001b[39m(auto_batch_size\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    504\u001b[0m \u001b[39m@set_skip_existing\u001b[39m(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    505\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    510\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m TensorDictBase:\n\u001b[1;32m--> 511\u001b[0m     tensordict_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_dist_params(tensordict, tensordict_out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    512\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m](tensordict_out, _requires_sample\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_requires_sample)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\probabilistic.py:487\u001b[0m, in \u001b[0;36mProbabilisticTensorDictSequential.get_dist_params\u001b[1;34m(self, tensordict, tensordict_out, **kwargs)\u001b[0m\n\u001b[0;32m    485\u001b[0m     \u001b[39mtype\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdefault_interaction_type\n\u001b[0;32m    486\u001b[0m \u001b[39mwith\u001b[39;00m set_interaction_type(\u001b[39mtype\u001b[39m):\n\u001b[1;32m--> 487\u001b[0m     \u001b[39mreturn\u001b[39;00m tds(tensordict, tensordict_out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:443\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    438\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt seems you tried to provide the parameters as an argument to the module when the module was not stateless. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    439\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf this is the case, this error should vanish by providing the parameters using the ``module(..., params=params)`` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    440\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msyntax.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 443\u001b[0m     \u001b[39mraise\u001b[39;00m err\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:431\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    430\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 431\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m), fun_name)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    432\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    433\u001b[0m         pattern \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.*takes \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ positional arguments but \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ were given|got multiple values for argument\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\common.py:273\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[1;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(out[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m dest)\n\u001b[0;32m    272\u001b[0m     \u001b[39mreturn\u001b[39;00m out[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m out\n\u001b[1;32m--> 273\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\_contextlib.py:126\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    125\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 126\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\utils.py:253\u001b[0m, in \u001b[0;36mset_skip_existing.__call__.<locals>.wrapper\u001b[1;34m(_self, tensordict, *args, **kwargs)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    248\u001b[0m     skip_existing()\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(key \u001b[39min\u001b[39;00m tensordict\u001b[39m.\u001b[39mkeys(\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m out_keys)\n\u001b[0;32m    250\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(key \u001b[39min\u001b[39;00m out_keys \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m in_keys)\n\u001b[0;32m    251\u001b[0m ):\n\u001b[0;32m    252\u001b[0m     \u001b[39mreturn\u001b[39;00m tensordict\n\u001b[1;32m--> 253\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\sequence.py:427\u001b[0m, in \u001b[0;36mTensorDictSequential.forward\u001b[1;34m(self, tensordict, tensordict_out, **kwargs)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(kwargs):\n\u001b[0;32m    426\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule:\n\u001b[1;32m--> 427\u001b[0m         tensordict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_module(module, tensordict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    428\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    429\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    430\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTensorDictSequential does not support keyword arguments other than \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensordict_out\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or in_keys: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_keys\u001b[39m}\u001b[39;00m\u001b[39m. Got \u001b[39m\u001b[39m{\u001b[39;00mkwargs\u001b[39m.\u001b[39mkeys()\u001b[39m}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    431\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\sequence.py:407\u001b[0m, in \u001b[0;36mTensorDictSequential._run_module\u001b[1;34m(self, module, tensordict, **kwargs)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_module\u001b[39m(\n\u001b[0;32m    399\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    400\u001b[0m     module: TensorDictModule,\n\u001b[0;32m    401\u001b[0m     tensordict: TensorDictBase,\n\u001b[0;32m    402\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    403\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    404\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartial_tolerant \u001b[39mor\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[0;32m    405\u001b[0m         key \u001b[39min\u001b[39;00m tensordict\u001b[39m.\u001b[39mkeys(include_nested\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39min_keys\n\u001b[0;32m    406\u001b[0m     ):\n\u001b[1;32m--> 407\u001b[0m         tensordict \u001b[39m=\u001b[39m module(tensordict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    408\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartial_tolerant \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(tensordict, LazyStackedTensorDict):\n\u001b[0;32m    409\u001b[0m         \u001b[39mfor\u001b[39;00m sub_td \u001b[39min\u001b[39;00m tensordict\u001b[39m.\u001b[39mtensordicts:\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:443\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    438\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt seems you tried to provide the parameters as an argument to the module when the module was not stateless. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    439\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf this is the case, this error should vanish by providing the parameters using the ``module(..., params=params)`` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    440\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msyntax.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 443\u001b[0m     \u001b[39mraise\u001b[39;00m err\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:431\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    430\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 431\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m), fun_name)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    432\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    433\u001b[0m         pattern \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.*takes \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ positional arguments but \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ were given|got multiple values for argument\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\common.py:273\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[1;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(out[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m dest)\n\u001b[0;32m    272\u001b[0m     \u001b[39mreturn\u001b[39;00m out[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m out\n\u001b[1;32m--> 273\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\_contextlib.py:126\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    125\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 126\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\utils.py:253\u001b[0m, in \u001b[0;36mset_skip_existing.__call__.<locals>.wrapper\u001b[1;34m(_self, tensordict, *args, **kwargs)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    248\u001b[0m     skip_existing()\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(key \u001b[39min\u001b[39;00m tensordict\u001b[39m.\u001b[39mkeys(\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m out_keys)\n\u001b[0;32m    250\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(key \u001b[39min\u001b[39;00m out_keys \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m in_keys)\n\u001b[0;32m    251\u001b[0m ):\n\u001b[0;32m    252\u001b[0m     \u001b[39mreturn\u001b[39;00m tensordict\n\u001b[1;32m--> 253\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\probabilistic.py:511\u001b[0m, in \u001b[0;36mProbabilisticTensorDictSequential.forward\u001b[1;34m(self, tensordict, tensordict_out, **kwargs)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[39m@dispatch\u001b[39m(auto_batch_size\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    504\u001b[0m \u001b[39m@set_skip_existing\u001b[39m(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    505\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    510\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m TensorDictBase:\n\u001b[1;32m--> 511\u001b[0m     tensordict_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_dist_params(tensordict, tensordict_out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    512\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m](tensordict_out, _requires_sample\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_requires_sample)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\probabilistic.py:487\u001b[0m, in \u001b[0;36mProbabilisticTensorDictSequential.get_dist_params\u001b[1;34m(self, tensordict, tensordict_out, **kwargs)\u001b[0m\n\u001b[0;32m    485\u001b[0m     \u001b[39mtype\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdefault_interaction_type\n\u001b[0;32m    486\u001b[0m \u001b[39mwith\u001b[39;00m set_interaction_type(\u001b[39mtype\u001b[39m):\n\u001b[1;32m--> 487\u001b[0m     \u001b[39mreturn\u001b[39;00m tds(tensordict, tensordict_out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:443\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    438\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt seems you tried to provide the parameters as an argument to the module when the module was not stateless. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    439\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf this is the case, this error should vanish by providing the parameters using the ``module(..., params=params)`` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    440\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msyntax.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 443\u001b[0m     \u001b[39mraise\u001b[39;00m err\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:431\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    430\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 431\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m), fun_name)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    432\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    433\u001b[0m         pattern \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.*takes \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ positional arguments but \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ were given|got multiple values for argument\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\common.py:273\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[1;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(out[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m dest)\n\u001b[0;32m    272\u001b[0m     \u001b[39mreturn\u001b[39;00m out[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m out\n\u001b[1;32m--> 273\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\_contextlib.py:126\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    125\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 126\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\utils.py:253\u001b[0m, in \u001b[0;36mset_skip_existing.__call__.<locals>.wrapper\u001b[1;34m(_self, tensordict, *args, **kwargs)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    248\u001b[0m     skip_existing()\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(key \u001b[39min\u001b[39;00m tensordict\u001b[39m.\u001b[39mkeys(\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m out_keys)\n\u001b[0;32m    250\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(key \u001b[39min\u001b[39;00m out_keys \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m in_keys)\n\u001b[0;32m    251\u001b[0m ):\n\u001b[0;32m    252\u001b[0m     \u001b[39mreturn\u001b[39;00m tensordict\n\u001b[1;32m--> 253\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\sequence.py:427\u001b[0m, in \u001b[0;36mTensorDictSequential.forward\u001b[1;34m(self, tensordict, tensordict_out, **kwargs)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(kwargs):\n\u001b[0;32m    426\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule:\n\u001b[1;32m--> 427\u001b[0m         tensordict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_module(module, tensordict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    428\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    429\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    430\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTensorDictSequential does not support keyword arguments other than \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensordict_out\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or in_keys: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_keys\u001b[39m}\u001b[39;00m\u001b[39m. Got \u001b[39m\u001b[39m{\u001b[39;00mkwargs\u001b[39m.\u001b[39mkeys()\u001b[39m}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    431\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\sequence.py:407\u001b[0m, in \u001b[0;36mTensorDictSequential._run_module\u001b[1;34m(self, module, tensordict, **kwargs)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_module\u001b[39m(\n\u001b[0;32m    399\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    400\u001b[0m     module: TensorDictModule,\n\u001b[0;32m    401\u001b[0m     tensordict: TensorDictBase,\n\u001b[0;32m    402\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    403\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    404\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartial_tolerant \u001b[39mor\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[0;32m    405\u001b[0m         key \u001b[39min\u001b[39;00m tensordict\u001b[39m.\u001b[39mkeys(include_nested\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39min_keys\n\u001b[0;32m    406\u001b[0m     ):\n\u001b[1;32m--> 407\u001b[0m         tensordict \u001b[39m=\u001b[39m module(tensordict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    408\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartial_tolerant \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(tensordict, LazyStackedTensorDict):\n\u001b[0;32m    409\u001b[0m         \u001b[39mfor\u001b[39;00m sub_td \u001b[39min\u001b[39;00m tensordict\u001b[39m.\u001b[39mtensordicts:\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:437\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    434\u001b[0m pattern \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mcompile(pattern)\n\u001b[0;32m    435\u001b[0m \u001b[39mif\u001b[39;00m pattern\u001b[39m.\u001b[39msearch(\u001b[39mstr\u001b[39m(err)) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(args[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], TensorDictBase):\n\u001b[0;32m    436\u001b[0m     \u001b[39m# this is raised whenever the module is an nn.Module (not a TensorDictModuleBase)\u001b[39;00m\n\u001b[1;32m--> 437\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    438\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt seems you tried to provide the parameters as an argument to the module when the module was not stateless. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    439\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf this is the case, this error should vanish by providing the parameters using the ``module(..., params=params)`` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    440\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msyntax.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    443\u001b[0m     \u001b[39mraise\u001b[39;00m err\n",
      "\u001b[1;31mTypeError\u001b[0m: It seems you tried to provide the parameters as an argument to the module when the module was not stateless. If this is the case, this error should vanish by providing the parameters using the ``module(..., params=params)`` syntax."
     ]
    }
   ],
   "source": [
    "print(\"Running policy:\", policy_module(base_env.RayCast()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_mm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\RL_UNITY\\full_car_envs\\my_experiment\\Assets\\subAssets\\Python\\environment\\Env5000\\TORCH_RL.ipynb Cell 34\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/TORCH_RL.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRunning value:\u001b[39m\u001b[39m\"\u001b[39m, value_module(base_env\u001b[39m.\u001b[39;49mreset()))\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:431\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    430\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 431\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m), fun_name)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    432\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    433\u001b[0m         pattern \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.*takes \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ positional arguments but \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ were given|got multiple values for argument\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\common.py:270\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[1;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[0;32m    263\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe key \u001b[39m\u001b[39m{\u001b[39;00mexpected_key\u001b[39m}\u001b[39;00m\u001b[39m wasn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt found in the keyword arguments \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    264\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut is expected to execute that function.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    265\u001b[0m             )\n\u001b[0;32m    266\u001b[0m tensordict \u001b[39m=\u001b[39m make_tensordict(\n\u001b[0;32m    267\u001b[0m     tensordict_values,\n\u001b[0;32m    268\u001b[0m     batch_size\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mSize([]) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_batch_size \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    269\u001b[0m )\n\u001b[1;32m--> 270\u001b[0m out \u001b[39m=\u001b[39m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    271\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(out[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m dest)\n\u001b[0;32m    272\u001b[0m \u001b[39mreturn\u001b[39;00m out[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\_contextlib.py:126\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    125\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 126\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\utils.py:253\u001b[0m, in \u001b[0;36mset_skip_existing.__call__.<locals>.wrapper\u001b[1;34m(_self, tensordict, *args, **kwargs)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    248\u001b[0m     skip_existing()\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(key \u001b[39min\u001b[39;00m tensordict\u001b[39m.\u001b[39mkeys(\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m out_keys)\n\u001b[0;32m    250\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(key \u001b[39min\u001b[39;00m out_keys \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m in_keys)\n\u001b[0;32m    251\u001b[0m ):\n\u001b[0;32m    252\u001b[0m     \u001b[39mreturn\u001b[39;00m tensordict\n\u001b[1;32m--> 253\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\common.py:810\u001b[0m, in \u001b[0;36mTensorDictModule.forward\u001b[1;34m(self, tensordict, tensordict_out, *args, **kwargs)\u001b[0m\n\u001b[0;32m    804\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[0;32m    805\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mSome tensors that are necessary for the module call may \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    806\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnot have not been found in the input tensordict: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    807\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthe following inputs are None: \u001b[39m\u001b[39m{\u001b[39;00mnone_set\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    808\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    809\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 810\u001b[0m         \u001b[39mraise\u001b[39;00m err\n\u001b[0;32m    811\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, (\u001b[39mdict\u001b[39m, TensorDictBase)):\n\u001b[0;32m    812\u001b[0m     tensors \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(tensors\u001b[39m.\u001b[39mget(key, \u001b[39mNone\u001b[39;00m) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_keys)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\common.py:798\u001b[0m, in \u001b[0;36mTensorDictModule.forward\u001b[1;34m(self, tensordict, tensordict_out, *args, **kwargs)\u001b[0m\n\u001b[0;32m    796\u001b[0m tensors \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(tensordict\u001b[39m.\u001b[39mget(in_key, \u001b[39mNone\u001b[39;00m) \u001b[39mfor\u001b[39;00m in_key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_keys)\n\u001b[0;32m    797\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 798\u001b[0m     tensors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_module(tensors, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    799\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    800\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(tensor \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m tensors) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mNone\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(err):\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\common.py:766\u001b[0m, in \u001b[0;36mTensorDictModule._call_module\u001b[1;34m(self, tensors, **kwargs)\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_module\u001b[39m(\n\u001b[0;32m    764\u001b[0m     \u001b[39mself\u001b[39m, tensors: Sequence[Tensor], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[0;32m    765\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor \u001b[39m|\u001b[39m Sequence[Tensor]:\n\u001b[1;32m--> 766\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule(\u001b[39m*\u001b[39;49mtensors, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    767\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:431\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    430\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 431\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m), fun_name)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    432\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    433\u001b[0m         pattern \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.*takes \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ positional arguments but \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ were given|got multiple values for argument\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tensordict\\nn\\functional_modules.py:431\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    430\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 431\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m), fun_name)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    432\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    433\u001b[0m         pattern \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.*takes \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ positional arguments but \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ were given|got multiple values for argument\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_mm)"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Running value:\", value_module(base_env.reset()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_env.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs import (\n",
    "    Compose,\n",
    "    DoubleToFloat,\n",
    "    ObservationNorm,\n",
    "    StepCounter,\n",
    "    TransformedEnv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    base_env,\n",
    "    Compose(\n",
    "        ObservationNorm(in_keys=base_env.observation_space),\n",
    "        DoubleToFloat(in_keys=base_env.observation_space),\n",
    "        StepCounter(),\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnv(unity_comms,5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs.utils import check_env_specs, ExplorationType, set_exploration_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = check_env_specs(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the frame skip frequency\n",
    "frame_skip_frequency = 5\n",
    "\n",
    "# Define the number of training steps\n",
    "total_timesteps = 100000\n",
    "\n",
    "# Define the directory paths\n",
    "LOG_DIR = './logs/'\n",
    "OPT_DIR = './opt_modeldata/'\n",
    "CHECKPOINT_DIR = './train_modeldata/'\n",
    "\n",
    "# Create the directories if they don't exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "os.makedirs(OPT_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import optuna for HPO\n",
    "import optuna\n",
    "# Import PPO for algos\n",
    "from stable_baselines3 import PPO\n",
    "# Evaluate Policy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "# Import wrappers\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "import os\n",
    "LOG_DIR = './logs/'\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)\n",
    "\n",
    "OPT_DIR = './opt_modeldata/'\n",
    "if not os.path.exists(OPT_DIR):\n",
    "    os.makedirs(OPT_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #https://github.com/araffin/rl-baselines-zoo/issues/29\n",
    "def optimize_ppo(trial):\n",
    "    \"\"\" Learning hyperparamters we want to optimise\"\"\"\n",
    "    return {\n",
    "        'n_steps': trial.suggest_int('n_steps', 2048, 20480),\n",
    "        'gamma': trial.suggest_loguniform('gamma', 0.8, 0.9999),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
    "        'clip_range': trial.suggest_uniform('clip_range', 0.1, 0.4),\n",
    "        'gae_lambda': trial.suggest_uniform('gae_lambda', 0.8, .99)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1024*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimize_agent(trial):\n",
    "    try:\n",
    "        model_params = optimize_ppo(trial)\n",
    "        env = UnityEnv(unity_comms,5000)\n",
    "        env = Monitor(env, LOG_DIR)\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        env = VecFrameStack(env, 4, channels_order='last')\n",
    "        model = PPO('MlpPolicy', env,batch_size=1024, tensorboard_log=LOG_DIR, verbose=0, **model_params)\n",
    "        model.learn(total_timesteps=20480)\n",
    "        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "        env.close()\n",
    "        SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(trial.number))\n",
    "        model.save(SAVE_PATH)\n",
    "        return mean_reward\n",
    "    except Exception as e: \n",
    "        return -1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(optimize_agent, n_trials=5, n_jobs=1)\n",
    "model_params = study.best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True\n",
    "\n",
    "CHECKPOINT_DIR = './train_modeldata/'\n",
    "if not os.path.exists(CHECKPOINT_DIR):\n",
    "    os.makedirs(CHECKPOINT_DIR)\n",
    "\n",
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)\n",
    "\n",
    "env = UnityEnv(unity_comms, 5000)\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')\n",
    "\n",
    "model = PPO('MlpPolicy', env, tensorboard_log=LOG_DIR, verbose=1, **model_params)\n",
    "\n",
    "# Adjust the total timesteps\n",
    "total_timesteps = 20480 \n",
    "\n",
    "model.learn(total_timesteps=total_timesteps, callback=callback)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
