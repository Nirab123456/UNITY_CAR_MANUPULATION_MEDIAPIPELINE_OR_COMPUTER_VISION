{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peaceful_pie.unity_comms import UnityComms\n",
    "import argparse\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Box, MultiBinary,Discrete\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers import FrameStack\n",
    "from collections import deque\n",
    "# Import UnityComms from peaceful_pie.unity_comms\n",
    "from peaceful_pie.unity_comms import UnityComms\n",
    "from peaceful_pie.unity_comms import UnityComms\n",
    "from peaceful_pie import ray_results_helper\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unity_comms(port: int):\n",
    "    unity_comms = UnityComms(port)\n",
    "    return unity_comms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "port = 5000  # Replace with your desired port number\n",
    "unity_comms_instance = unity_comms(port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unity_comms_instance \n",
    "unity_comms = unity_comms_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVector3:\n",
    "    def __init__(self, x, y, z):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.z = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "class UnityEnv(Env):\n",
    "    def __init__(self, unity_comms, i):\n",
    "        self.unity_comms = unity_comms\n",
    "        self.action_space = Discrete(15)  # Scale the action range to -1.0 to 1.0\n",
    "        self.observation_space = Box(low=-np.inf, high=np.inf, shape=(3,), dtype=np.float32)\n",
    "        self.initial_position = None\n",
    "        self.prev_position = None\n",
    "        self.prev_velocity = 0\n",
    "        self.initial_get_CarCollisionDetected = 0\n",
    "        self.frame_count = 0\n",
    "        self.obstacle_max = 0\n",
    "        self.reward_max = 0\n",
    "        self.i = i\n",
    "        self.reward_modifier = self.ModifyReward()\n",
    "        self.loop_counter = 0\n",
    "        self.save_counter = 0\n",
    "        self.modified_reward = 0\n",
    "        self.global_step = 0\n",
    "        self.summary_writer = SummaryWriter()\n",
    "        self.reward_modifier.global_step = self.global_step\n",
    "        self.reward_modifier.summary_writer = self.summary_writer\n",
    "\n",
    "    class ModifyReward:\n",
    "        def __init__(self):\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(4, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 1),\n",
    "                nn.Tanh()\n",
    "            ).to(self.device)\n",
    "            #load model\n",
    "            self.model.load_state_dict(torch.load('D:/RL_UNITY/full_car_envs/my_experiment/Assets/subAssets/Python/environment/Env5000/mini_model/mini_model_5000_6'))\n",
    "            self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001)\n",
    "            self.dataset = []\n",
    "            self.y_prde = None\n",
    "            self.global_step = 0\n",
    "            self.summary_writer = None\n",
    "\n",
    "        def train(self):\n",
    "            dataloader = DataLoader(self.dataset)\n",
    "            for obs, action, rewards in dataloader:\n",
    "                obs, action, rewards = obs.to(self.device), action.to(self.device), rewards.to(self.device)\n",
    "                inputs = torch.cat((obs.squeeze(0), action), dim=1)\n",
    "                self.y_prde = self.model(inputs)\n",
    "                loss = nn.MSELoss()(self.y_prde, rewards.unsqueeze(1))\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.summary_writer.add_scalar(\"Loss\", loss.item(), global_step=self.global_step)\n",
    "                self.global_step += 1\n",
    "\n",
    "            return self.y_prde.item()\n",
    "\n",
    "        def store(self, obs, action, rewards):\n",
    "            obs = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "            action = torch.tensor(action, dtype=torch.float32).unsqueeze(0)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(0)\n",
    "            self.dataset.append((obs, action, rewards))\n",
    "\n",
    "        def clear(self):\n",
    "            self.dataset.clear()\n",
    "\n",
    "    def step(self, action):\n",
    "        # Perform the action based on the provided action index\n",
    "        if action == 0:\n",
    "            getattr(self.unity_comms, f\"GoForward_{self.i}\")()\n",
    "        elif action == 1:\n",
    "            getattr(self.unity_comms, f\"GoReverse_{self.i}\")()\n",
    "        elif action == 2:\n",
    "            getattr(self.unity_comms, f\"TurnLeft_{self.i}\")()\n",
    "        elif action == 3:\n",
    "            getattr(self.unity_comms, f\"TurnRight_{self.i}\")()\n",
    "        elif action == 4:\n",
    "            getattr(self.unity_comms, f\"Handbrake_{self.i}\")()\n",
    "        elif action == 5:\n",
    "            getattr(self.unity_comms, f\"GoForward_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"TurnLeft_{self.i}\")()\n",
    "        elif action == 6:\n",
    "            getattr(self.unity_comms, f\"GoForward_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"TurnRight_{self.i}\")()\n",
    "        elif action == 7:\n",
    "            getattr(self.unity_comms, f\"GoForward_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"Handbrake_{self.i}\")()\n",
    "        elif action == 8:\n",
    "            getattr(self.unity_comms, f\"GoReverse_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"TurnLeft_{self.i}\")()\n",
    "        elif action == 9:\n",
    "            getattr(self.unity_comms, f\"GoReverse_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"TurnRight_{self.i}\")()\n",
    "        elif action == 10:\n",
    "            getattr(self.unity_comms, f\"GoReverse_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"Handbrake_{self.i}\")()\n",
    "        elif action == 11:\n",
    "            getattr(self.unity_comms, f\"GoForward_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"TurnLeft_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"Handbrake_{self.i}\")()\n",
    "        elif action == 12:\n",
    "            getattr(self.unity_comms, f\"GoForward_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"TurnRight_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"Handbrake_{self.i}\")()\n",
    "        elif action == 13:\n",
    "            getattr(self.unity_comms, f\"GoReverse_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"TurnLeft_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"Handbrake_{self.i}\")()\n",
    "        elif action == 14:\n",
    "            getattr(self.unity_comms, f\"GoReverse_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"TurnRight_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"Handbrake_{self.i}\")()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action index\")\n",
    "\n",
    "        self.prev_position = self.Get_position()\n",
    "        self.prev_velocity = self.Get_velocity()\n",
    "\n",
    "        position = self.Get_position()\n",
    "        velocity = self.Get_velocity()\n",
    "        # Concatenate the position and velocity to form the observation\n",
    "        self.observation = np.array([position[0], position[1], velocity], dtype=np.float32)\n",
    "\n",
    "        reward = self.Get_reward()\n",
    "        done = self.done()\n",
    "\n",
    "        # Update the info dictionary with relevant information\n",
    "        info = {\n",
    "            'episode_reward': reward,\n",
    "            'episode_length': self.frame_count,\n",
    "            'current_observation': self.observation,  # Fix the variable name\n",
    "            'action_taken': action,\n",
    "            'obstacle_visible': self.obstacle_Collision_detected(),\n",
    "            'goal_reached': self.Goal(),\n",
    "            'car_collision_detected': self.Get_carCollisionDetected(),\n",
    "            'Real_reward': self.pre_reward,\n",
    "            'modified_reward': self.modified_reward,\n",
    "            'loop_counter': self.loop_counter\n",
    "        }\n",
    "\n",
    "        self.reward_modifier.store(self.observation, action, reward)\n",
    "        self.modified_reward=self.reward_modifier.train()  # Train the model before getting modified reward\n",
    "        store = self.reward_modifier.store\n",
    "        self.reward_modifier.clear()\n",
    "\n",
    "\n",
    "        MINI_MODEL_DIR = './mini_model/'\n",
    "        if not os.path.exists(MINI_MODEL_DIR):\n",
    "            os.makedirs(MINI_MODEL_DIR)\n",
    "\n",
    "            \n",
    "        self.loop_counter += 1\n",
    "\n",
    "        if self.loop_counter % 1000 == 0:\n",
    "            self.save_counter += 1\n",
    "            model_name = f\"mini_model_{self.i}_{self.save_counter}\"\n",
    "            model_path = os.path.join(MINI_MODEL_DIR, model_name)\n",
    "            torch.save(self.reward_modifier.model.state_dict(), model_path)\n",
    "            print(f\"Model saved as {model_name}\")        \n",
    "            # Write save_counter value to TensorBoard\n",
    "            self.summary_writer.add_scalar(\"Save Counter\", self.save_counter, global_step=self.global_step)\n",
    "\n",
    "        return self.observation, reward, done, info\n",
    "    \n",
    "    \n",
    "    def Get_reward(self):\n",
    "        obstacle_penalty = -0.7  # Penalty for encountering an obstacle\n",
    "        reward_bonus = 0.7  # Reward for finding a reward object\n",
    "        goal_reward = 1  # Reward for reaching the goal\n",
    "        collision_penalty = -1.0  # Penalty for collision\n",
    "        car_collision_penalty = -1.0  # Penalty for colliding with the car\n",
    "        velocity_reward = 0.1  # Reward for moving forward\n",
    "\n",
    "\n",
    "        reward = 0.0\n",
    "        if self.weight_comparision():\n",
    "            reward += obstacle_penalty\n",
    "        else:\n",
    "            reward += reward_bonus\n",
    "\n",
    "        reward += self.Step_panalty()\n",
    "\n",
    "        if self.Goal():\n",
    "            reward += goal_reward\n",
    "\n",
    "        if self.obstacle_Collision_detected():\n",
    "            reward += collision_penalty\n",
    "\n",
    "        if self.Get_carCollisionDetected():\n",
    "            reward += car_collision_penalty\n",
    "\n",
    "        if self.Check_valocity_increment():\n",
    "            reward += velocity_reward\n",
    "\n",
    "        self.pre_reward = reward\n",
    "\n",
    "        modified_reward = self.modified_reward\n",
    "        return modified_reward\n",
    "    \n",
    "\n",
    "\n",
    "    def RayCast(self):\n",
    "        ray_results = getattr(self.unity_comms, f\"GetRayCastsResults_{self.i}\")()\n",
    "        distance = np.array(ray_results['rayDistances'], dtype=np.float32)\n",
    "        types = np.array(ray_results['rayHitObjectTypes'], dtype=np.int32)\n",
    "        num_types = ray_results['NumObjectTypes']\n",
    "        self.actual_result = ray_results_helper.ray_results_to_feature_np(\n",
    "            ray_results_helper.RayResults(\n",
    "                NumObjectTypes=num_types,\n",
    "                rayDistances=distance,\n",
    "                rayHitObjectTypes=types,\n",
    "            )\n",
    "        )  \n",
    "\n",
    "        return self.actual_result \n",
    "    \n",
    "\n",
    "    def obstacle_visible(self):\n",
    "        actual_result=self.RayCast()\n",
    "        obstacle_channel = actual_result[0]  # Extract the obstacle channel from the actual result\n",
    "        return obstacle_channel\n",
    "\n",
    "    def reward_visible(self):\n",
    "        actual_result=self.RayCast()\n",
    "        reward_channel = actual_result[1]  # Extract the reward channel from the actual result\n",
    "        return reward_channel\n",
    "    \n",
    "    def weight_comparision(self):\n",
    "\n",
    "        obstacle_channel = self.obstacle_visible()\n",
    "        reward_channel = self.reward_visible()\n",
    "        self.obstacle_weight = np.mean(obstacle_channel)\n",
    "        self.reward_weight = np.mean(reward_channel)\n",
    "\n",
    "        if self.obstacle_weight > self.reward_weight:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def Goal(self):\n",
    "        reward_channel = self.reward_visible()\n",
    "        reward_max = np.max(reward_channel)\n",
    "        if reward_max >= 1:\n",
    "            return True\n",
    "        \n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def obstacle_Collision_detected(self):\n",
    "        obstacle_channel = self.obstacle_visible()\n",
    "        obstacle_max = np.max(obstacle_channel)\n",
    "        if obstacle_max >= 1:\n",
    "            return True\n",
    "        \n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        self.RayCast()\n",
    "        Reset_Car = getattr(self.unity_comms, f\"ResetPosition_{self.i}\")()  \n",
    "        Reset_Agent = getattr(self.unity_comms, f\"ResetPosition_Plane_{self.i}\")()\n",
    "        position =self.Get_position()\n",
    "        position = np.array(position, dtype=np.float32)\n",
    "        self.frame_count=0\n",
    "        self.reward_modifier.clear()\n",
    "        return position\n",
    "\n",
    "    def done(self):\n",
    "        if self.Get_carCollisionDetected() == True:\n",
    "            self.frame_count=0\n",
    "            return True\n",
    "        elif self.Get_movingPlaneCollision() == True:\n",
    "            self.frame_count=0\n",
    "            return True\n",
    "        elif self.Get_planeCollision() == True:\n",
    "            self.frame_count=0\n",
    "            return True\n",
    "        elif self.Check_stuck() == True:\n",
    "            self.frame_count=0\n",
    "            return True\n",
    "        else:\n",
    "            self.frame_count += 1\n",
    "            return False\n",
    "\n",
    "    def Get_carCollisionDetected(self):\n",
    "        collision_count = 0\n",
    "        collision = collision = getattr(self.unity_comms, f\"CarCollisionDetected_{self.i}\")()\n",
    "        if collision == 1:\n",
    "            collision_count += 1\n",
    "            if collision_count >=2:\n",
    "                collision_count = 0\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    def Get_movingPlaneCollision(self):\n",
    "        collision_count = 0\n",
    "        collision = getattr(self.unity_comms, f\"GetMovingPlaneCollision_{self.i}\")()\n",
    "        if collision == 1:\n",
    "            collision_count += 1\n",
    "            if collision_count >= 1:\n",
    "                collision_count = 0\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    def Get_planeCollision(self):\n",
    "        collision_count = 0\n",
    "        collision = getattr(self.unity_comms, f\"GetPlaneCollision_{self.i}\")()\n",
    "        if collision == 1:\n",
    "            collision_count += 1\n",
    "            if collision_count > 0:\n",
    "                collision_count = 0\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    def Get_rewardCollision(self):\n",
    "        collision = getattr(self.unity_comms, f\"GetRewardCollision_{self.i}\")()\n",
    "        self.rewardcollision = collision\n",
    "\n",
    "    def Step_panalty(self):\n",
    "        if self.done() == True:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "    def Get_velocity(self):\n",
    "        valocity =  getattr(self.unity_comms, f\"CarSpeedUI_{self.i}\")()\n",
    "        return valocity\n",
    "\n",
    "    def Check_valocity_increment(self):\n",
    "        valocity = self.Get_velocity()\n",
    "        if valocity > self.prev_velocity:\n",
    "            self.prev_velocity = valocity\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def Check_stuck(self):\n",
    "        position_threshold = 0.01\n",
    "        consecutive_steps = 40\n",
    "        position_counter = 0\n",
    "        \n",
    "        if self.prev_position is None:\n",
    "            self.prev_position = self.get_position()\n",
    "\n",
    "        for _ in range(consecutive_steps):\n",
    "            x, y, z = self.Get_position()\n",
    "\n",
    "            position_diff = np.linalg.norm(np.array([x, y, z]) - np.array(self.prev_position))\n",
    "            if position_diff < position_threshold:\n",
    "                position_counter += 1\n",
    "            else:\n",
    "                position_counter = 0\n",
    "\n",
    "            self.prev_position = [x, y, z]\n",
    "\n",
    "        if position_counter >= consecutive_steps:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def Get_position(self):\n",
    "        position = getattr(self.unity_comms, f\"GetPosition_{self.i}\")()\n",
    "        # Extract x, y, and z components from position\n",
    "        x = position['x']\n",
    "        y = position['y']\n",
    "        z = position['z']\n",
    "        return x, y, z\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2445.68    ,  -19.841257, -115.21    ], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = UnityEnv(unity_comms,5000)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 1, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "observation, reward, done, info = env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'episode_reward': 1.7,\n",
       " 'episode_length': 0,\n",
       " 'current_observation': array([ 2.445680e+03, -2.067210e+01,  7.100961e-01], dtype=float32),\n",
       " 'action_taken': 7,\n",
       " 'obstacle_visible': False,\n",
       " 'goal_reached': True,\n",
       " 'car_collision_detected': False,\n",
       " 'Real_reward': 1.7,\n",
       " 'modified_reward': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import optuna for HPO\n",
    "import optuna\n",
    "# Import PPO for algos\n",
    "from stable_baselines3 import PPO\n",
    "# Evaluate Policy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "# Import wrappers\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "import os\n",
    "LOG_DIR = './logs/'\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)\n",
    "\n",
    "OPT_DIR = './opt_modeldata/'\n",
    "if not os.path.exists(OPT_DIR):\n",
    "    os.makedirs(OPT_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the frame skip frequency\n",
    "frame_skip_frequency = 5\n",
    "\n",
    "# Define the number of training steps\n",
    "total_timesteps = 100000\n",
    "\n",
    "# Define the directory paths\n",
    "LOG_DIR = './logs/'\n",
    "OPT_DIR = './opt_modeldata/'\n",
    "CHECKPOINT_DIR = './train_modeldata/'\n",
    "\n",
    "# Create the directories if they don't exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "os.makedirs(OPT_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Import os for file path management\n",
    "import os \n",
    "# Import Base Callback for saving models\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True\n",
    "    \n",
    "CHECKPOINT_DIR = './train_modeldata/'\n",
    "if not os.path.exists(CHECKPOINT_DIR):\n",
    "    os.makedirs(CHECKPOINT_DIR)\n",
    "\n",
    "callback = TrainAndLoggingCallback(check_freq=3000, save_path=CHECKPOINT_DIR)\n",
    "\n",
    "env = UnityEnv(unity_comms,5000)\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to ./logs/PPO_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 4096, but because the `RolloutBuffer` is of size `n_steps * n_envs = 500`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 500\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=500 and n_envs=1)\n",
      "  warnings.warn(\n",
      "c:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 1, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.1      |\n",
      "|    ep_rew_mean     | 2.99     |\n",
      "| time/              |          |\n",
      "|    fps             | 2        |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 245      |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.62          |\n",
      "|    ep_rew_mean          | 6.19          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1             |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 530           |\n",
      "|    total_timesteps      | 1000          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.7984842e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -2.71         |\n",
      "|    explained_variance   | -0.365        |\n",
      "|    learning_rate        | 4.46e-05      |\n",
      "|    loss                 | 3.87          |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.00159      |\n",
      "|    value_loss           | 8.02          |\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "model_params = {\n",
    "    'n_steps': 500,\n",
    "    'gamma': 0.839729258597705,\n",
    "    'learning_rate': 4.456287316411457e-05,\n",
    "    'clip_range': 0.29999077694725984,\n",
    "    'gae_lambda': 0.9074598206209127\n",
    "}\n",
    "\n",
    "\n",
    "model = PPO('MlpPolicy', env,batch_size=4096, tensorboard_log=LOG_DIR, verbose=1, **model_params)\n",
    "\n",
    "model.learn(total_timesteps=total_timesteps, callback=callback)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
